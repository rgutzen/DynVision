@article{Abdelhack2018_,
  title = {Sharpening of {{Hierarchical Visual Feature Representations}} of {{Blurred Images}}},
  author = {Abdelhack, Mohamed and Kamitani, Yukiyasu},
  year = {2018},
  month = may,
  journal = {eNeuro},
  volume = {5},
  number = {3},
  publisher = {Society for Neuroscience},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0443-17.2018},
  urldate = {2024-10-19},
  chapter = {New Research},
  langid = {english},
  pmid = {29756028},
  keywords = {Decoding,Deep Neural Network,fMRI},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/IE65I86F/Abdelhack2018_.pdf}
}

@article{Ahmadian2013_1994,
  title = {Analysis of the {{Stabilized Supralinear Network}}},
  author = {Ahmadian, Yashar and Rubin, Daniel B. and Miller, Kenneth D.},
  year = {2013},
  month = aug,
  journal = {Neural Computation},
  volume = {25},
  number = {8},
  pages = {1994--2037},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00472},
  urldate = {2024-10-19},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/4S2B72YR/Ahmadian2013_1994.pdf;/home/rgutzen/Cloud/own/Zotero/storage/3HDNXE9C/Analysis-of-the-Stabilized-Supralinear-Network.html}
}

@misc{Balwani2025_,
  title = {Constructing {{Biologically Constrained RNNs}} via {{Dale}}'s {{Backprop}} and {{Topologically-Informed Pruning}}},
  author = {Balwani, Aishwarya H. and Wang, Alex Q. and Najafi, Farzaneh and Choi, Hannah},
  year = {2025},
  month = jan,
  publisher = {Neuroscience},
  doi = {10.1101/2025.01.09.632231},
  urldate = {2025-01-21},
  abstract = {Recurrent neural networks (RNNs) have emerged as a prominent tool for modeling cortical function, and yet their conventional architecture is lacking in physiological and anatomical fidelity. In particular, these models often fail to incorporate two crucial biological constraints: i) Dale's law, i.e., sign constraints that preserve the ``type'' of projections from individual neurons, and ii) Structured connectivity motifs, i.e., highly sparse yet defined connections amongst various neuronal populations. Both constraints are known to impair learning performance in artificial neural networks, especially when trained to perform complicated tasks; but as modern experimental methodologies allow us to record from diverse neuronal populations spanning multiple brain regions, using RNN models to study neuronal interactions without incorporating these fundamental biological properties raises questions regarding the validity of the insights gleaned from them. To address these concerns, our work develops methods that let us train RNNs which respect Dale's law whilst simultaneously maintaining a specific sparse connectivity pattern across the entire network. We provide mathematical grounding and guarantees for our approaches incorporating both types of constraints, and show empirically that our models match the performance of RNNs trained without any constraints. Finally, we demonstrate the utility of our methods for inferring multi-regional interactions by training RNN models of the cortical network to reconstruct 2-photon calcium imaging data during visual behaviour in mice, whilst enforcing data-driven, cell-type specific connectivity constraints between various neuronal populations spread across multiple cortical layers and brain areas. In doing so, we find that the interactions inferred by our model corroborate experimental findings in agreement with the theory of predictive coding, thus validating the applicability of our methods.},
  archiveprefix = {Neuroscience},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/BA75LMJJ/Balwani et al. - 2025 - Constructing Biologically Constrained RNNs via Dale’s Backprop and Topologically-Informed Pruning.pdf}
}

@misc{Bashivan2023_,
  title = {Learning {{Robust Kernel Ensembles}} with {{Kernel Average Pooling}}},
  author = {Bashivan, Pouya and Ibrahim, Adam and Dehghani, Amirozhan and Ren, Yifei},
  year = {2023},
  month = may,
  number = {arXiv:2210.00062},
  eprint = {2210.00062},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-12},
  abstract = {Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pool (KAP), a new neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when combined with activation noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial improvements in robustness against strong adversarial attacks such as AutoAttack that are on par with adversarially trained networks but are importantly obtained without training on any adversarial examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/AHMUP6Q7/Bashivan et al. - 2023 - Learning Robust Kernel Ensembles with Kernel Avera.pdf}
}

@article{Bastos2015_390,
  ids = {Bastos2015_390a},
  title = {Visual {{Areas Exert Feedforward}} and {{Feedback Influences}} through {{Distinct Frequency Channels}}},
  author = {Bastos, Andr{\'e} Moraes and Vezoli, Julien and Bosman, Conrado Arturo and Schoffelen, Jan-Mathijs and Oostenveld, Robert and Dowdall, Jarrod Robert and De Weerd, Peter and Kennedy, Henry and Fries, Pascal},
  year = {2015},
  month = jan,
  journal = {Neuron},
  volume = {85},
  number = {2},
  pages = {390--401},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2014.12.018},
  urldate = {2022-06-28},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/S6KKFCYW/Bastos2015_390.pdf;/home/rgutzen/Cloud/own/Zotero/storage/48HQMPVT/S089662731401099X.html}
}

@misc{Brands2024_2024.07.26.605075,
  title = {Contrast-Dependent Response Modulation in Convolutional Neural Networks Captures Behavioral and Neural Signatures of Visual Adaptation},
  author = {Brands, Amber Marijn and Oz, Zilan and Vuk{\v s}i{\'c}, Nikolina and Ortiz, Paulo and Groen, Iris Isabelle Anna},
  year = {2024},
  month = jul,
  primaryclass = {New Results},
  pages = {2024.07.26.605075},
  publisher = {bioRxiv},
  doi = {10.1101/2024.07.26.605075},
  urldate = {2024-08-12},
  abstract = {Human perception remains robust under challenging viewing conditions. Robust perception is thought to be facilitated by nonlinear response properties, including temporal adaptation (reduced responses to re-peated stimuli) and contrast gain (shift in the contrast response function with pre-exposure to a stimulus). Temporal adaptation and contrast gain have both been shown to aid object recognition, however, their joint effect on perceptual and neural responses remains unclear. Here, we collected behavioural measurements and electrocorticography (EEG) data while human participants (both sexes) classified objects embedded within temporally repeated noise patterns, whereby object contrast was varied. Our findings reveal an in-teraction effect, with increased categorization performance as a result of temporal adaptation for higher but not lower contrast stimuli. This increase in behavioral performance after adaptation is associated with more pronounced contrast-dependent modulation of evoked neural responses, as well as better decoding of object information from EEG activity. To elucidate the neural computations underlying these effects, we endowed deep convolutional neural networks (DCNN) with various temporal adaptation mechanisms, including intrinsic suppression and temporal divisive normalisation. We demonstrate that incorporating a biologically-inspired contrast response function to modify temporal adaptation helps DCNNs to accurately capture human behaviour and neural activation profiles. Moreover, we find that networks with multiplicative temporal adaptation mechanisms, such as divisive normalization, show higher robustness against spatial shifts in the inputs compared to DCNNs employing additive mechanisms. Overall, we reveal how interaction effects between nonlinear response properties influence human perception in challenging viewing contexts and investigate potential computations that mediate these effects. Significance statement Humans are able to perceive the environment even when viewing conditions are suboptimal. This robust perception has been linked to nonlinear neural processing of incoming visual information. Here, we examine the joint impact of two neural response properties, temporal adaptation and contrast gain, during object recognition, demonstrating interaction effects on categorization performance and in evoked neural responses. Using convolutional neural networks, we investigate various temporal adaptation mechanisms mediating the neural responses and perception, demonstrating that introducing contrast-dependent modulation of the unit activations captures human behaviour and neural object representations. Our findings shed light on how neural response properties give rise to robust perception and offer a framework to study the underlying neural dynamics and their impact on perception.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/GPXHXYJC/Brands2024_2024.07.26.605075.pdf}
}

@inproceedings{Butkus2024_,
  title = {Recurrent Attentional Selection Can Explain Flexible Trading of Accuracy and Energy in Biological Vision},
  booktitle = {{{CCN}} 2024},
  author = {Butkus, Eivinas and Ying, Zhuofan and Chen, Peiyu and Kriegeskorte, Nikolaus},
  year = {2024},
  address = {Boston, MA}
}

@article{Celeghin2023_,
  title = {Convolutional Neural Networks for Vision Neuroscience: Significance, Developments, and Outstanding Issues},
  shorttitle = {Convolutional Neural Networks for Vision Neuroscience},
  author = {Celeghin, Alessia and Borriero, Alessio and Orsenigo, Davide and Diano, Matteo and M{\'e}ndez Guerrero, Carlos Andr{\'e}s and Perotti, Alan and Petri, Giovanni and Tamietto, Marco},
  year = {2023},
  month = jul,
  journal = {Frontiers in Computational Neuroscience},
  volume = {17},
  publisher = {Frontiers},
  issn = {1662-5188},
  doi = {10.3389/fncom.2023.1153572},
  urldate = {2025-02-18},
  abstract = {{$<$}p{$>$}Convolutional Neural Networks (CNN) are a class of machine learning models predominately used in computer vision tasks and can achieve human-like performance through learning from experience. Their striking similarities to the structural and functional principles of the primate visual system allow for comparisons between these artificial networks and their biological counterparts, enabling exploration of how visual functions and neural representations may emerge in the real brain from a limited set of computational principles. After considering the basic features of CNNs, we discuss the opportunities and challenges of endorsing CNNs as {$<$}italic{$>$}in silico{$<$}/italic{$>$} models of the primate visual system. Specifically, we highlight several emerging notions about the anatomical and physiological properties of the visual system that still need to be systematically integrated into current CNN models. These tenets include the implementation of parallel processing pathways from the early stages of retinal input and the reconsideration of several assumptions concerning the serial progression of information flow. We suggest design choices and architectural constraints that could facilitate a closer alignment with biology provide causal evidence of the predictive link between the artificial and biological visual systems. Adopting this principled perspective could potentially lead to new research questions and applications of CNNs beyond modeling object recognition.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Blindsight,Convolutional Neural Networks - CNN,Pulvinar,Superior colliculus (SC),V1-independent vision,ventral stream,Visual System},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/3HVXAARN/Celeghin et al. - 2023 - Convolutional neural networks for vision neuroscience significance, developments, and outstanding i.pdf}
}

@inproceedings{Choksi2020_,
  title = {Brain-Inspired Predictive Coding Dynamics Improve the Robustness of Deep Neural Networks},
  booktitle = {{{NeurIPS}} 2020 {{Workshop SVRHM}}},
  author = {Choksi, Bhavin and Mozafari, Milad and O'May, Callum Biggs and Ador, B. and Alamia, Andrea and VanRullen, Rufin},
  year = {2020},
  month = oct,
  urldate = {2024-10-18},
  abstract = {Deep neural networks excel at image classification, but their performance is far less robust to input perturbations than human perception. In this work we address this shortcoming by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We augment a pretrained feedforward classification model (VGG16 trained on ImageNet) with a ``predictive coding'' strategy: a framework popular in neuroscience for characterizing cortical function. At each layer of the hierarchical model, generative feedback ``predicts'' (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network's representations across timesteps, and to optimize the network's feedback weights over the natural image dataset--a form of unsupervised training. We demonstrate that this results in a network with improved robustness compared to the corresponding feedforward baseline, not only against various types of noise but also against a suite of adversarial attacks. We propose that most feedforward models could be equipped with these brain-inspired feedback dynamics, thus improving their robustness to input perturbations.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/XRGEZPSD/Choksi2020_.pdf}
}

@inproceedings{Chollet2017_1800,
  title = {Xception: {{Deep Learning}} with {{Depthwise Separable Convolutions}}},
  shorttitle = {Xception},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chollet, Francois},
  year = {2017},
  month = jul,
  pages = {1800--1807},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.195},
  urldate = {2025-02-19},
  abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This obsedvmtion leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/7PA3MIF5/Chollet - 2017 - Xception Deep Learning with Depthwise Separable Convolutions.pdf}
}

@misc{Costa2023_2023.04.25.538251,
  title = {Unlocking the {{Secrets}} of the {{Primate Visual Cortex}}: {{A CNN-Based Approach Traces}} the {{Origins}} of {{Major Organizational Principles}} to {{Retinal Sampling}}},
  shorttitle = {Unlocking the {{Secrets}} of the {{Primate Visual Cortex}}},
  author = {da Costa, Danny and Kornemann, Lukas and Goebel, Rainer and Senden, Mario},
  year = {2023},
  month = apr,
  primaryclass = {New Results},
  pages = {2023.04.25.538251},
  publisher = {bioRxiv},
  doi = {10.1101/2023.04.25.538251},
  urldate = {2024-10-16},
  abstract = {Primate visual cortex exhibits key organizational principles: Cortical magnification, eccentricity-dependent receptive field size and spatial frequency tuning as well as radial bias. We provide compelling evidence that these principles arise from the interplay of the non-uniform distribution of retinal ganglion cells (RGCs), and a quasi-uniform convergence rate from the retina to the cortex. We show that convolutional neural networks (CNNs) outfitted with a retinal sampling layer, which resamples images according to retinal ganglion cell density, develop these organizational principles. Surprisingly, our results indicate that radial bias is spatial-frequency dependent and only manifests for high spatial frequencies. For low spatial frequencies, the bias shifts towards orthogonal orientations. These findings introduce a novel hypothesis about the origin of radial bias. Quasi-uniform convergence limits the range of spatial frequencies (in retinal space) that can be resolved, while retinal sampling determines the spatial frequency content throughout the retina.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/QYMEEIYC/Costa2023_2023.04.25.538251.pdf}
}

@article{Covelo2025_102719,
  title = {Spatiotemporal Network Dynamics and Structural Correlates in the Human Cerebral Cortex {\emph{in Vitro}}},
  author = {Covelo, Joana and Camassa, Alessandra and {Sanchez-Sanchez}, Jose Manuel and Manasanch, Arnau and Porta, Leonardo Dalla and {Cancino-Fuentes}, Nathalia and {Barbero-Castillo}, Almudena and Robles, Rita M. and Bosch, Miquel and {Tapia-Gonzalez}, Silvia and {Merino-Serrais}, Paula and Carre{\~n}o, Mar and {Conde-Blanco}, Estefania and Arboix, Jordi Rumi{\`a} and Rold{\'a}n, Pedro and DeFelipe, Javier and {Sanchez-Vives}, Maria V.},
  year = {2025},
  month = mar,
  journal = {Progress in Neurobiology},
  volume = {246},
  pages = {102719},
  issn = {0301-0082},
  doi = {10.1016/j.pneurobio.2025.102719},
  urldate = {2025-02-05},
  abstract = {Elucidating human cerebral cortex function is essential for understanding the physiological basis of both healthy and pathological brain states. We obtained extracellular local field potential recordings from slices of neocortical tissue from refractory epilepsy patients. Multi-electrode recordings were combined with histological information, providing a two-dimensional spatiotemporal characterization of human cortical dynamics in control conditions and following modulation of the excitation/inhibition balance. Slices expressed spontaneous rhythmic activity consistent with slow wave activity, comprising alternating active (Up) and silent (Down) states (Up-duration: 0.08\,{\textpm}\,0.03\,s, Down-duration: 2.62\,{\textpm}\,2.12\,s, frequency: 0.75\,{\textpm}\,0.39\,Hz). Up states propagated from deep to superficial layers, with faster propagation speeds than in other species (vertical: 64.6\,mm/s; horizontal: 65.9\,mm/s). GABAA blockade progressively transformed the emergent activity into epileptiform discharges, marked by higher firing rates, faster network recruitment and propagation, and infraslow rhythmicity (0.01\,Hz). This dynamical characterization broadens our understanding of the mechanistic organization of the human cortical network at the micro- and mesoscale.},
  keywords = {Cortical columns,Cortical dynamics,Epilepsy surgery,Granger causality,Pharmacoresistant,Slow oscillations,Slow waves},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/DR9MJKBL/Covelo et al. - 2025 - Spatiotemporal network dynamics and structural correlates in the human cerebral cortex in vitro.pdf;/home/rgutzen/Cloud/own/Zotero/storage/BM9JIJM9/S0301008225000103.html}
}

@misc{Falcon2020_,
  title = {{{PyTorchLightning}}/Pytorch-Lightning: 0.7.6 Release},
  shorttitle = {{{PyTorchLightning}}/Pytorch-Lightning},
  author = {Falcon, William and Borovec, Jirka and W{\"a}lchli, Adrian and Eggert, Nic and Schock, Justus and Jordan, Jeremy and Skafte, Nicki and Ir1dXD and Bereznyuk, Vadim and Harris, Ethan and Murrell, Tullie and Yu, Peter and Pr{\ae}sius, Sebastian and Addair, Travis and Zhong, Jacob and Lipin, Dmitry and Uchida, So and Bapat, Shreyas and Schr{\"o}ter, Hendrik and Dayma, Boris and Karnachev, Alexey and Kulkarni, Akshay and Komatsu, Shunta and Martin.B and SCHIRATTI, Jean-Baptiste and Mary, Hadrien and Byrne, Donal and Eyzaguirre, Cristobal and {cinjon} and Bakhtin, Anton},
  year = {2020},
  month = may,
  doi = {10.5281/zenodo.3828935},
  urldate = {2025-02-21},
  abstract = {The lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate},
  howpublished = {Zenodo},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/MQQTW37N/3828935.html}
}

@inproceedings{Fang2021_2661,
  title = {Incorporating {{Learnable Membrane Time Constant To Enhance Learning}} of {{Spiking Neural Networks}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timoth{\'e}e and Huang, Tiejun and Tian, Yonghong},
  year = {2021},
  pages = {2661--2671},
  urldate = {2024-11-24},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/W8GPAQAG/Fang et al. - 2021 - Incorporating Learnable Membrane Time Constant To Enhance Learning of Spiking Neural Networks.pdf}
}

@article{Groen2022_7562,
  title = {Temporal {{Dynamics}} of {{Neural Responses}} in {{Human Visual Cortex}}},
  author = {Groen, Iris I. A. and Piantoni, Giovanni and Montenegro, Stephanie and Flinker, Adeen and Devore, Sasha and Devinsky, Orrin and Doyle, Werner and Dugan, Patricia and Friedman, Daniel and Ramsey, Nick F. and Petridou, Natalia and Winawer, Jonathan},
  year = {2022},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {42},
  number = {40},
  pages = {7562--7580},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1812-21.2022},
  urldate = {2024-07-05},
  abstract = {Neural responses to visual stimuli exhibit complex temporal dynamics, including subadditive temporal summation, response reduction with repeated or sustained stimuli (adaptation), and slower dynamics at low contrast. These phenomena are often studied independently. Here, we demonstrate these phenomena within the same experiment and model the underlying neural computations with a single computational model. We extracted time-varying responses from electrocorticographic recordings from patients presented with stimuli that varied in duration, interstimulus intedvml (ISI) and contrast. Aggregating data across patients from both sexes yielded 98 electrodes with robust visual responses, covering both earlier (V1--V3) and higher-order (V3a/b, LO, TO, IPS) retinotopic maps. In all regions, the temporal dynamics of neural responses exhibit several nonlinear features. Peak response amplitude saturates with high contrast and longer stimulus durations, the response to a second stimulus is suppressed for short ISIs and recovers for longer ISIs, and response latency decreases with increasing contrast. These features are accurately captured by a computational model composed of a small set of canonical neuronal operations, that is, linear filtering, rectification, exponentiation, and a delayed divisive normalization. We find that an increased normalization term captures both contrast- and adaptation-related response reductions, suggesting potentially shared underlying mechanisms. We additionally demonstrate both changes and invariance in temporal response dynamics between earlier and higher-order visual areas. Together, our results reveal the presence of a wide range of temporal and contrast-dependent neuronal dynamics in the human visual cortex and demonstrate that a simple model captures these dynamics at millisecond resolution. SIGNIFICANCE STATEMENT Sensory inputs and neural responses change continuously over time. It is especially challenging to understand a system that has both dynamic inputs and outputs. Here, we use a computational modeling approach that specifies computations to convert a time-varying input stimulus to a neural response time course, and we use this to predict neural activity measured in the human visual cortex. We show that this computational model predicts a wide variety of complex neural response shapes, which we induced experimentally by manipulating the duration, repetition, and contrast of visual stimuli. By comparing data and model predictions, we uncover systematic properties of temporal dynamics of neural signals, allowing us to better understand how the brain processes dynamic sensory information.},
  chapter = {Research Articles},
  copyright = {Copyright {\copyright} 2022 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {35999054},
  keywords = {adaptation,broadband,contrast,ECOG,temporal dynamics,vision},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/AVY6STS4/Groen2022_7562.pdf}
}

@article{Heeger2019_22783,
  title = {Oscillatory Recurrent Gated Neural Integrator Circuits ({{ORGaNICs}}), a Unifying Theoretical Framework for Neural Dynamics},
  author = {Heeger, David J. and Mackey, Wayne E.},
  year = {2019},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {45},
  pages = {22783--22794},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1911633116},
  urldate = {2024-10-18},
  abstract = {Working memory is an example of a cognitive and neural process that is not static but evolves dynamically with changing sensory inputs; another example is motor preparation and execution. We introduce a theoretical framework for neural dynamics, based on oscillatory recurrent gated neural integrator circuits (ORGaNICs), and apply it to simulate key phenomena of working memory and motor control. The model circuits simulate neural activity with complex dynamics, including sequential activity and traveling waves of activity, that manipulate (as well as maintain) information during working memory. The same circuits convert spatial patterns of premotor activity to temporal profiles of motor control activity and manipulate (e.g., time warp) the dynamics. Derivative-like recurrent connectivity, in particular, serves to manipulate and update internal models, an essential feature of working memory and motor execution. In addition, these circuits incorporate recurrent normalization, to ensure stability over time and robustness with respect to perturbations of synaptic weights.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/68UY2WJ2/Heeger2019_22783.pdf}
}

@article{Heeger2020_22494,
  title = {A Recurrent Circuit Implements Normalization, Simulating the Dynamics of {{V1}} Activity},
  author = {Heeger, David J. and Zemlianova, Klavdia O.},
  year = {2020},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {36},
  pages = {22494--22505},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2005417117},
  urldate = {2024-10-18},
  abstract = {The normalization model has been applied to explain neural activity in diverse neural systems including primary visual cortex (V1). The model's defining characteristic is that the response of each neuron is divided by a factor that includes a weighted sum of activity of a pool of neurons. Despite the success of the normalization model, there are three unresolved issues. 1) Experimental evidence supports the hypothesis that normalization in V1 operates via recurrent amplification, i.e., amplifying weak inputs more than strong inputs. It is unknown how normalization arises from recurrent amplification. 2) Experiments have demonstrated that normalization is weighted such that each weight specifies how one neuron contributes to another's normalization pool. It is unknown how weighted normalization arises from a recurrent circuit. 3) Neural activity in V1 exhibits complex dynamics, including gamma oscillations, linked to normalization. It is unknown how these dynamics emerge from normalization. Here, a family of recurrent circuit models is reported, each of which comprises coupled neural integrators to implement normalization via recurrent amplification with arbitrary normalization weights, some of which can recapitulate key experimental obsedvmtions of the dynamics of neural activity in V1.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/U2QFADE9/Heeger2020_22494.pdf}
}

@article{Helfrich2019_82,
  title = {Neural Entrainment and Network Resonance in Support of Top-down Guided Attention},
  author = {Helfrich, Randolph F and Breska, Assaf and Knight, Robert T},
  year = {2019},
  month = oct,
  journal = {Current Opinion in Psychology},
  series = {Attention \& {{Perception}}},
  volume = {29},
  pages = {82--89},
  issn = {2352-250X},
  doi = {10.1016/j.copsyc.2018.12.016},
  urldate = {2023-08-16},
  abstract = {Which neural mechanisms provide the functional basis of top-down guided cognitive control? Here, we review recent evidence that suggest that the neural basis of attention is inherently rhythmic. In particular, we discuss two physical properties of self-sustained networks, namely entrainment and resonance, and how these shape the timescale of attentional control. Several recent findings revealed theta-band (3--8\,Hz) dynamics in top-down guided behavior. These reports were paralleled by intracranial recordings, which implicated theta oscillations in the organization of functional attention networks. We discuss how the intrinsic network architecture shapes covert attentional sampling as well as overt behavior. Taken together, we posit that theta rhythmicity is an inherent feature of the attention network in support of top-down guided goal-directed behavior.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/5GDGTXJ7/Helfrich2019_82.pdf;/home/rgutzen/Cloud/own/Zotero/storage/AD6XCXJ9/S2352250X18301611.html}
}

@article{Hu2025_3436,
  title = {Relationship between Functional Structures and Horizontal Connections in Macaque Inferior Temporal Cortex},
  author = {Hu, Danling and Sato, Takayuki and Rockland, Kathleen S. and Tanifuji, Manabu and Tanigawa, Hisashi},
  year = {2025},
  month = jan,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {3436},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-87517-3},
  urldate = {2025-02-19},
  abstract = {Horizontal connections in anterior inferior temporal cortex (ITC) are thought to play an important role in object recognition by integrating information across spatially separated functional columns, but their functional organization remains unclear. Using a combination of optical imaging, electrophysiological recording, and anatomical tracing, we investigated the relationship between stimulus-response maps and patterns of horizontal axon terminals in the macaque ITC. In contrast to the ``like-to-like'' connectivity observed in the early visual cortex, we found that horizontal axons in ITC do not preferentially connect sites with similar object selectivity. While some axon terminal patches shared responsiveness to specific visual features with the injection site, many connected to regions with different selectivity. Our results suggest that horizontal connections in anterior ITC exhibit diverse functional connectivity, potentially supporting flexible integration of visual information for advanced object recognition processes.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/5RT4Z2H3/Hu et al. - 2025 - Relationship between functional structures and horizontal connections in macaque inferior temporal c.pdf}
}

@article{Hupe1998_784,
  title = {Cortical Feedback Improves Discrimination between Figure and Background by {{V1}}, {{V2}} and {{V3}} Neurons},
  author = {Hup{\'e}, J. M. and James, A. C. and Payne, B. R. and Lomber, S. G. and Girard, P. and Bullier, J.},
  year = {1998},
  month = aug,
  journal = {Nature},
  volume = {394},
  number = {6695},
  pages = {784--787},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/29537},
  urldate = {2024-10-18},
  abstract = {A single visual stimulus activates neurons in many different cortical areas. A major challenge in cortical physiology is to understand how the neural activity in these numerous active zones leads to a unified percept of the visual scene. The anatomical basis for these interactions is the dense network of connections that link the visual areas. Within this network, feedforward connections transmit signals from lower-order areas such as V1 or V2 to higher-order areas. In addition, there is a dense web of feedback connections which, despite their anatomical prominence1,2,3,4, remain functionally mysterious5,6,7,8. Here we show, using reversible inactivation of a higher-order area (monkey area V5/MT), that feedback connections serve to amplify and focus activity of neurons in lower-order areas, and that they are important in the differentiation of figure from ground, particularly in the case of stimuli of low visibility. More specifically, we show that feedback connections facilitate responses to objects moving within the classical receptive field; enhance suppression evoked by background stimuli in the surrounding region; and have the strongest effects for stimuli of low salience.},
  copyright = {1998 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/USDAKWT5/Hupe1998_784.pdf}
}

@article{Issa2018_e42870,
  title = {Neural Dynamics at Successive Stages of the Ventral Visual Stream Are Consistent with Hierarchical Error Signals},
  author = {Issa, Elias B and Cadieu, Charles F and DiCarlo, James J},
  editor = {Connor, Ed and Marder, Eve and Connor, Ed},
  year = {2018},
  month = nov,
  journal = {eLife},
  volume = {7},
  pages = {e42870},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.42870},
  urldate = {2024-11-08},
  abstract = {Ventral visual stream neural responses are dynamic, even for static image presentations. However, dynamical neural models of visual cortex are lacking as most progress has been made modeling static, time-averaged responses. Here, we studied population neural dynamics during face detection across three cortical processing stages. Remarkably,{\textasciitilde}30 milliseconds after the initially evoked response, we found that neurons in intermediate level areas decreased their responses to typical configurations of their preferred face parts relative to their response for atypical configurations even while neurons in higher areas achieved and maintained a preference for typical configurations. These hierarchical neural dynamics were inconsistent with standard feedforward circuits. Rather, recurrent models computing prediction errors between stages captured the observed temporal signatures. This model of neural dynamics, which simply augments the standard feedforward model of online vision, suggests that neural responses to static images may encode top-down prediction errors in addition to bottom-up feature estimates.},
  keywords = {face recognition,neural dynamics,neurophysiology,visual cortex},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/8B6ZYTM2/Issa et al. - 2018 - Neural dynamics at successive stages of the ventra.pdf}
}

@article{Kaleb_,
  title = {Feedback Control Guides Credit Assignment in Recurrent Neural Networks},
  author = {Kaleb, Klara and Feulner, Barbara and Gallego, Juan A and Clopath, Claudia},
  abstract = {How do brain circuits learn to generate behaviour? While significant strides have been made in understanding learning in artificial neural networks, applying this knowledge to biological networks remains challenging. For instance, while backpropagation is known to perform accurate credit assignment of error in artificial neural networks, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear. One of the major challenges is that the brain's extensive recurrent connectivity requires the propagation of error through both space and time, a problem that is notoriously difficult to solve in vanilla recurrent neural networks. Moreover, the extensive feedback connections in the brain are known to influence forward network activity, but the interaction between feedback-driven activity changes and local, synaptic plasticity-based learning is not fully understood. Building on our previous work modelling motor learning, this work investigates the mechanistic properties of pre-trained networks with feedback control on a standard motor task. We show that feedback control of the ongoing recurrent network dynamics approximates the optimal first-order gradient with respect to the network activities, allowing for rapid, ongoing movement correction. Moreover, we show that trial-by-trial adaptation to a persistent perturbation using a local, biologically plausible learning rule that integrates recent activity and error feedback is both more accurate and more efficient with feedback control during learning, due to the decoupling of the recurrent network dynamics and the injection of an adaptive, second-order gradient into the network dynamics. Thus, our results suggest that feedback control may guide credit assignment in biological recurrent neural networks, enabling both rapid and efficient learning in the brain.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/YJSK9WLB/Kaleb et al. - Feedback control guides credit assignment in recurrent neural networks.pdf}
}

@article{Kar2019_974,
  title = {Evidence That Recurrent Circuits Are Critical to the Ventral Stream's Execution of Core Object Recognition Behavior},
  author = {Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B. and DiCarlo, James J.},
  year = {2019},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {22},
  number = {6},
  pages = {974--983},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0392-5},
  urldate = {2025-02-13},
  abstract = {Non-recurrent deep convolutional neural networks (CNNs) are currently the best at modeling core object recognition, a behavior that is supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (IT) cortex. If recurrence is critical to this behavior, then primates should outperform feedforward-only deep CNNs for images that require additional recurrent processing beyond the feedforward IT response. Here we first used behavioral methods to discover hundreds of these `challenge' images. Second, using large-scale electrophysiology, we observed that behaviorally sufficient object identity solutions emerged {\textasciitilde}30\,ms later in the IT cortex for challenge images compared with primate performance-matched `control' images. Third, these behaviorally critical late-phase IT response patterns were poorly predicted by feedforward deep CNN activations. Notably, very-deep CNNs and shallower recurrent CNNs better predicted these late IT responses, suggesting that there is a functional equivalence between additional nonlinear transformations and recurrence. Beyond arguing that recurrent circuits are critical for rapid object identification, our results provide strong constraints for future recurrent model development.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Neural decoding,Neural encoding,Object vision},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/2X6DJ6S7/Kar et al. - 2019 - Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recog.pdf}
}

@article{Keck2017_20160158,
  title = {Integrating {{Hebbian}} and Homeostatic Plasticity: The Current State of the Field and Future Research Directions},
  shorttitle = {Integrating {{Hebbian}} and Homeostatic Plasticity},
  author = {Keck, Tara and Toyoizumi, Taro and Chen, Lu and Doiron, Brent and Feldman, Daniel E. and Fox, Kevin and Gerstner, Wulfram and Haydon, Philip G. and H{\"u}bener, Mark and Lee, Hey-Kyoung and Lisman, John E. and Rose, Tobias and Sengpiel, Frank and Stellwagen, David and Stryker, Michael P. and Turrigiano, Gina G. and {van Rossum}, Mark C.},
  year = {2017},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {372},
  number = {1715},
  pages = {20160158},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2016.0158},
  urldate = {2025-02-21},
  abstract = {We summarize here the results presented and subsequent discussion from the meeting on Integrating Hebbian and Homeostatic Plasticity at the Royal Society in April 2016. We first outline the major themes and results presented at the meeting. We next provide a synopsis of the outstanding questions that emerged from the discussion at the end of the meeting and finally suggest potential directions of research that we believe are most promising to develop an understanding of how these two forms of plasticity interact to facilitate functional changes in the brain. This article is part of the themed issue `Integrating Hebbian and homeostatic plasticity'.},
  keywords = {Hebbian plasticity,homeostatic mechanisms,theoretical modelling},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/8PYNNR9L/Keck et al. - 2017 - Integrating Hebbian and homeostatic plasticity the current state of the field and future research d.pdf}
}

@article{Kietzmann2019_21854,
  title = {Recurrence Is Required to Capture the Representational Dynamics of the Human Visual System},
  author = {Kietzmann, Tim C. and Spoerer, Courtney J. and S{\"o}rensen, Lynn K. A. and Cichy, Radoslaw M. and Hauk, Olaf and Kriegeskorte, Nikolaus},
  year = {2019},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {43},
  pages = {21854--21863},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1905544116},
  urldate = {2023-12-04},
  abstract = {The human visual system is an intricate network of brain regions that enables us to recognize the world around us. Despite its abundant lateral and feedback connections, object processing is commonly viewed and studied as a feedforward process. Here, we measure and model the rapid representational dynamics across multiple stages of the human ventral stream using time-resolved brain imaging and deep learning. We observe substantial representational transformations during the first 300 ms of processing within and across ventral-stream regions. Categorical divisions emerge in sequence, cascading forward and in reverse across regions, and Granger causality analysis suggests bidirectional information flow between regions. Finally, recurrent deep neural network models clearly outperform parameter-matched feedforward models in terms of their ability to capture the multiregion cortical dynamics. Targeted virtual cooling experiments on the recurrent deep network models further substantiate the importance of their lateral and top-down connections. These results establish that recurrent models are required to understand information processing in the human ventral stream.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/BJGQJJYZ/Kietzmann2019_21854.pdf}
}

@misc{Kim2020_,
  title = {Disentangling Neural Mechanisms for Perceptual Grouping},
  author = {Kim, Junkyung and Linsley, Drew and Thakkar, Kalpit and Serre, Thomas},
  year = {2020},
  month = oct,
  number = {arXiv:1906.01558},
  eprint = {1906.01558},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.01558},
  urldate = {2024-10-18},
  abstract = {Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level "Gestalt" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/RRQUKJ4I/Kim2020_.pdf;/home/rgutzen/Cloud/own/Zotero/storage/HLQLYV2Y/1906.html}
}

@article{Kreiman2020_222,
  title = {Beyond the Feedforward Sweep: Feedback Computations in the Visual Cortex},
  shorttitle = {Beyond the Feedforward Sweep},
  author = {Kreiman, Gabriel and Serre, Thomas},
  year = {2020},
  journal = {Annals of the New York Academy of Sciences},
  volume = {1464},
  number = {1},
  pages = {222--241},
  issn = {1749-6632},
  doi = {10.1111/nyas.14320},
  urldate = {2024-03-18},
  abstract = {Visual perception involves the rapid formation of a coarse image representation at the onset of visual processing, which is iteratively refined by late computational processes. These early versus late time windows approximately map onto feedforward and feedback processes, respectively. State-of-the-art convolutional neural networks, the main engine behind recent machine vision successes, are feedforward architectures. Their successes and limitations provide critical information regarding which visual tasks can be solved by purely feedforward processes and which require feedback mechanisms. We provide an overview of recent work in cognitive neuroscience and machine vision that highlights the possible role of feedback processes for both visual recognition and beyond. We conclude by discussing important open questions for future research.},
  copyright = {{\copyright} 2020 New York Academy of Sciences.},
  langid = {english},
  keywords = {categorization,deep learning,grouping,machine vision,neural networks,visual reasoning},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/ZGCKB7BP/Kreiman2020_222.pdf;/home/rgutzen/Cloud/own/Zotero/storage/FKUJER2E/nyas.html}
}

@inproceedings{Krizhevsky2012_,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-12-05},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/I6Y7U7BR/Krizhevsky2012_.pdf}
}

@misc{Kubilius2018_408385,
  title = {{{CORnet}}: {{Modeling}} the {{Neural Mechanisms}} of {{Core Object Recognition}}},
  shorttitle = {{{CORnet}}},
  author = {Kubilius, Jonas and Schrimpf, Martin and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L. K. and DiCarlo, James J.},
  year = {2018},
  month = sep,
  primaryclass = {New Results},
  pages = {408385},
  publisher = {bioRxiv},
  doi = {10.1101/408385},
  urldate = {2024-05-24},
  abstract = {Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NAS-Net architectures, demonstrating increasingly better object categorization performance and increasingly better explanatory power of both neural and behavioral responses. However, from the neuroscientist's point of view, the relationship between such very deep architectures and the ventral visual pathway is incomplete in at least two ways. On the one hand, current state-of-the-art ANNs appear to be too complex (e.g., now over 100 levels) compared with the relatively shallow cortical hierarchy (4-8 levels), which makes it difficult to map their elements to those in the ventral visual stream and to understand what they are doing. On the other hand, current state-of-the-art ANNs appear to be not complex enough in that they lack recurrent connections and the resulting neural response dynamics that are commonplace in the ventral visual stream. Here we describe our ongoing efforts to resolve both of these issues by developing a ``CORnet'' family of deep neural network architectures. Rather than just seeking high object recognition performance (as the state-of-the-art ANNs above), we instead try to reduce the model family to its most important elements and then gradually build new ANNs with recurrent and skip connections while monitoring both performance and the match between each new CORnet model and a large body of primate brain and behavioral data. We report here that our current best ANN model derived from this approach (CORnet-S) is among the top models on Brain-Score, a composite benchmark for comparing models to the brain, but is simpler than other deep ANNs in terms of the number of convolutions performed along the longest path of information processing in the model. All CORnet models are available at github.com/dicarlolab/CORnet, and we plan to up-date this manuscript and the available models in this family as they are produced.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/4H7UXSYT/Kubilius2018_408385.pdf}
}

@article{Lappalainen2024_1132,
  title = {Connectome-Constrained Networks Predict Neural Activity across the Fly Visual System},
  author = {Lappalainen, Janne K. and Tschopp, Fabian D. and Prakhya, Sridhama and McGill, Mason and Nern, Aljoscha and Shinomiya, Kazunori and Takemura, Shin-ya and Gruntman, Eyal and Macke, Jakob H. and Turaga, Srinivas C.},
  year = {2024},
  month = oct,
  journal = {Nature},
  volume = {634},
  number = {8036},
  pages = {1132--1140},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07939-3},
  urldate = {2024-11-24},
  abstract = {We can now measure the connectivity of every neuron in a neural circuit1--9, but we cannot measure other biological details, including the dynamical characteristics of each neuron. The degree to which measurements of connectivity alone can inform the understanding of neural computation is an open question10. Here we show that with experimental measurements of only the connectivity of a biological neural network, we can predict the neural activity underlying a specified neural computation. We constructed a model neural network with the experimentally determined connectivity for 64 cell types in the motion pathways of the fruit fly optic lobe1--5 but with unknown parameters for the single-neuron and single-synapse properties. We then optimized the values of these unknown parameters using techniques from deep learning11, to allow the model network to detect visual motion12. Our mechanistic model makes detailed, experimentally testable predictions for each neuron in the connectome. We found that model predictions agreed with experimental measurements of neural activity across 26 studies. Our work demonstrates a strategy for generating detailed hypotheses about the mechanisms of neural circuit function from connectivity measurements. We show that this strategy is more likely to be successful when neurons are sparsely connected---a universally observed feature of biological neural networks across species and brain regions.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Motion detection,Network models,Neural circuits},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/65RSJIRL/Lappalainen et al. - 2024 - Connectome-constrained networks predict neural activity across the fly visual system.pdf}
}

@inproceedings{Leclerc2023_12011,
  title = {{{FFCV}}: {{Accelerating Training}} by {{Removing Data Bottlenecks}}},
  shorttitle = {{{FFCV}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Leclerc, Guillaume and Ilyas, Andrew and Engstrom, Logan and Park, Sung Min and Salman, Hadi and M{\k a}dry, Aleksander},
  year = {2023},
  pages = {12011--12020},
  urldate = {2025-02-21},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/XBCQ625X/Leclerc et al. - 2023 - FFCV Accelerating Training by Removing Data Bottlenecks.pdf}
}

@inproceedings{Liang2015_3367,
  title = {Recurrent Convolutional Neural Network for Object Recognition},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Liang, Ming and Hu, Xiaolin},
  year = {2015},
  month = jun,
  pages = {3367--3375},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2015.7298958},
  urldate = {2025-02-19},
  abstract = {In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.},
  keywords = {Computational modeling,Convolution,Feeds},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/Z6KG4ZIH/Liang and Hu - 2015 - Recurrent convolutional neural network for object recognition.pdf;/home/rgutzen/Cloud/own/Zotero/storage/48YXL53E/7298958.html}
}

@inproceedings{Lindsay2019_,
  title = {Do {{Biologically-Realistic Recurrent Architectures Produce Biologically-Realistic Models}}?},
  booktitle = {2019 {{Conference}} on {{Cognitive Computational Neuroscience}}},
  author = {Lindsay, Grace W. and Moskovitz, Theodore and Yang, Guangyu Robert and Miller, Kenneth},
  year = {2019},
  publisher = {Cognitive Computational Neuroscience},
  address = {Berlin, Germany},
  doi = {10.32470/CCN.2019.1175-0},
  urldate = {2024-09-06},
  abstract = {Many details are known about microcircuitry in visual cortices. For example, neurons have supralinear activation functions, they're either excitatory (E) or inhibitory (I), connection strengths fall off with distance, and the output cells of an area are excitatory. This circuitry is important as it's believed to support core functions such as normalization and surround suppression. Yet, multi-area models of the visual processing stream don't usually include these details. Here, we introduce known-features of recurrent processing into the architecture of a convolutional neural network and observe how connectivity and activity change as a result. We find that certain E-I differences found in data emerge in the models, though the details depend on which architectural elements are included. We also compare the representations learned by these models to data, and perform analyses on the learned weight structures to assess the nature of the neural interactions.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/UF8D5ZQH/Lindsay et al. - 2019 - Do Biologically-Realistic Recurrent Architectures .pdf}
}

@misc{Lindsay2020_2019.12.13.875534,
  title = {A Unified Circuit Model of Attention: {{Neural}} and Behavioral Effects},
  shorttitle = {A Unified Circuit Model of Attention},
  author = {Lindsay, Grace W. and Rubin, Daniel B. and Miller, Kenneth D.},
  year = {2020},
  month = jul,
  primaryclass = {New Results},
  pages = {2019.12.13.875534},
  publisher = {bioRxiv},
  doi = {10.1101/2019.12.13.875534},
  urldate = {2023-08-29},
  abstract = {Selective visual attention modulates neural activity in the visual system in complex ways and leads to enhanced performance on difficult visual tasks. Here, we show that a simple circuit model, the stabilized supralinear network, gives a unified account of a wide variety of effects of attention on neural responses. We replicate results from studies of both feature and spatial attention, addressing findings in a variety of experimental paradigms on changes both in firing rates and in correlated neural variability. Finally, we expand this circuit model into an architecture that can perform visual tasks---a convolutional neural network---in order to show that these neural effects can enhance detection performance. This work provides the first unified mechanistic account of the effects of attention on neural and behavioral responses.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/FQALED4R/Lindsay2020_2019.12.13.875534.pdf}
}

@article{Lindsay2021_2017,
  title = {Convolutional {{Neural Networks}} as a {{Model}} of the {{Visual System}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Convolutional {{Neural Networks}} as a {{Model}} of the {{Visual System}}},
  author = {Lindsay, Grace W.},
  year = {2021},
  month = sep,
  journal = {Journal of Cognitive Neuroscience},
  volume = {33},
  number = {10},
  pages = {2017--2031},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01544},
  urldate = {2023-11-29},
  abstract = {Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNs in vision research beyond basic object recognition.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/CSHHJBEJ/Lindsay2021_2017.pdf;/home/rgutzen/Cloud/own/Zotero/storage/B6NGAS8A/97402.html}
}

@techreport{Lindsay2022_,
  type = {Preprint},
  title = {Bio-Inspired Neural Networks Implement Different Recurrent Visual Processing Strategies than Task-Trained Ones Do},
  author = {Lindsay, Grace W. and {Mrsic-Flogel}, Thomas D. and Sahani, Maneesh},
  year = {2022},
  month = mar,
  institution = {Neuroscience},
  doi = {10.1101/2022.03.07.483196},
  urldate = {2023-11-30},
  abstract = {Behavioral studies suggest that recurrence in the visual system is important for processing degraded stimuli. There are two broad anatomical forms this recurrence can take, lateral or feedback, each with different assumed functions. Here we add four different kinds of recurrence---two of each anatomical form---to a feedforward convolutional neural network and find all forms capable of increasing the ability of the network to classify noisy digit images. Specifically, we take inspiration from findings in biology by adding predictive feedback and lateral surround suppression. To compare these forms of recurrence to anatomically-matched counterparts we also train feedback and lateral connections directly to classify degraded images. Counter-intuitively, we find that the anatomy of the recurrence is not related to its function: both forms of task-trained recurrence change neural activity and behavior similarly to each other and differently from their bio-inspired anatomical counterparts. By using several analysis tools frequently applied to neural data, we identified the distinct strategies used by the predictive versus task-trained networks. Specifically, predictive feedback de-noises the representation of noisy images at the first layer of the network and decreases its dimensionality, leading to an expected increase in classification performance. Surprisingly, in the task-trained networks, representations are not de-noised over time at the first layer (in fact, they become `noiser' and dimensionality increases) yet these dynamics do lead to de-noising at later layers. The analyses used here can be applied to real neural recordings to identify the strategies at play in the brain. Our analysis of an fMRI dataset weakly supports the predictive feedback model but points to a need for higher-resolution cross-regional data to understand recurrent visual processing. .},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/WBP99WC8/Lindsay et al. - 2022 - Bio-inspired neural networks implement different r.pdf}
}

@misc{Lindsey2019_,
  title = {A {{Unified Theory}} of {{Early Visual Representations}} from {{Retina}} to {{Cortex}} through {{Anatomically Constrained Deep CNNs}}},
  author = {Lindsey, Jack and Ocko, Samuel A. and Ganguli, Surya and Deny, Stephane},
  year = {2019},
  month = jan,
  number = {arXiv:1901.00945},
  eprint = {1901.00945},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.00945},
  urldate = {2025-01-07},
  abstract = {The visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex, typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that, for simple cortical networks, visual representations at the retinal output emerge as nonlinear and lossy feature detectors, whereas they emerge as linear and faithful encoders of the visual scene for more complex cortices. This result predicts that the retinas of small vertebrates should perform sophisticated nonlinear computations, extracting features directly relevant to behavior, whereas retinas of large animals such as primates should mostly encode the visual scene linearly and respond to a much broader range of stimuli. These predictions could reconcile the two seemingly incompatible views of the retina as either performing feature extraction or efficient coding of natural scenes, by suggesting that all vertebrates lie on a spectrum between these two objectives, depending on the degree of neural resources allocated to their visual system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/JKNF8ELY/Lindsey et al. - 2019 - A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrai.pdf;/home/rgutzen/Cloud/own/Zotero/storage/MQ2MNL3N/1901.html}
}

@article{Linsley_,
  title = {Stable and Expressive Recurrent Vision Models},
  author = {Linsley, Drew and Ashok, Alekh K and Govindarajan, Lakshmi N and Liu, Rex and Serre, Thomas},
  abstract = {Primate vision depends on recurrent processing for reliable perception [1--3]. A growing body of literature also suggests that recurrent connections improve the learning efficiency and generalization of vision models on classic computer vision challenges. Why then, are current large-scale challenges dominated by feedforward networks? We posit that the effectiveness of recurrent vision models is bottlenecked by the standard algorithm used for training them, ``back-propagation through time'' (BPTT), which has O(N ) memory-complexity for training an N step model. Thus, recurrent vision model design is bounded by memory constraints, forcing a choice between rivaling the enormous capacity of leading feedforward models or trying to compensate for this deficit through granular and complex dynamics. Here, we develop a new learning algorithm, ``contractor recurrent back-propagation'' (C-RBP), which alleviates these issues by achieving constant O(1) memory-complexity with steps of recurrent processing. We demonstrate that recurrent vision models trained with C-RBP can detect long-range spatial dependencies in a synthetic contour tracing task that BPTT-trained models cannot. We further show that recurrent vision models trained with C-RBP to solve the large-scale Panoptic Segmentation MS-COCO challenge outperform the leading feedforward approach, with fewer free parameters. C-RBP is a general-purpose learning algorithm for any application that can benefit from expansive recurrent dynamics. Code and data are available at https://github.com/c-rbp.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/VDD7SS28/Linsley et al. - Stable and expressive recurrent vision models.pdf}
}

@inproceedings{Linsley2020_10456,
  title = {Stable and Expressive Recurrent Vision Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Linsley, Drew and Karkada Ashok, Alekh and Govindarajan, Lakshmi Narasimhan and Liu, Rex and Serre, Thomas},
  year = {2020},
  volume = {33},
  pages = {10456--10467},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-02-21},
  abstract = {Primate vision depends on recurrent processing for reliable perception. A growing body of literature also suggests that recurrent connections improve the learning efficiency and generalization of vision models on classic computer vision challenges. Why then, are current large-scale challenges dominated by feedforward networks? We posit that the effectiveness of recurrent vision models is bottlenecked by the standard algorithm used for training them, "back-propagation through time" (BPTT), which has O(N) memory-complexity for training an N step model. Thus, recurrent vision model design is bounded by memory constraints, forcing a choice between rivaling the enormous capacity of leading feedforward models or trying to compensate for this deficit through granular and complex dynamics. Here, we develop a new learning algorithm, "contractor recurrent back-propagation" (C-RBP), which alleviates these issues by achieving constant O(1) memory-complexity with steps of recurrent processing. We demonstrate that recurrent vision models trained with C-RBP can detect long-range spatial dependencies in a synthetic contour tracing task that BPTT-trained models cannot. We further show that recurrent vision models trained with C-RBP to solve the large-scale Panoptic Segmentation MS-COCO challenge outperform the leading feedforward approach, with fewer free parameters. C-RBP is a general-purpose learning algorithm for any application that can benefit from expansive recurrent dynamics. Code and data are available at https://github.com/c-rbp.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/VCR7AXM5/Linsley et al. - 2020 - Stable and expressive recurrent vision models.pdf}
}

@article{Lotter2020_210,
  title = {A Neural Network Trained for Prediction Mimics Diverse Features of Biological Neurons and Perception},
  author = {Lotter, William and Kreiman, Gabriel and Cox, David},
  year = {2020},
  month = apr,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {4},
  pages = {210--219},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0170-9},
  urldate = {2025-02-13},
  abstract = {Recent work has shown that convolutional neural networks (CNNs) trained on image recognition tasks can serve as valuable models for predicting neural responses in primate visual cortex. However, these models typically require biologically infeasible levels of labelled training data, so this similarity must at least arise via different paths. In addition, most popular CNNs are solely feedforward, lacking a notion of time and recurrence, whereas neurons in visual cortex produce complex time-varying responses, even to static inputs. Towards addressing these inconsistencies with biology, here we study the emergent properties of a recurrent generative network that is trained to predict future video frames in a self-supervised manner. Remarkably, the resulting model is able to capture a wide variety of seemingly disparate phenomena observed in visual cortex, ranging from single-unit response dynamics to complex perceptual motion illusions, even when subjected to highly impoverished stimuli. These results suggest potentially deep connections between recurrent predictive neural network models and computations in the brain, providing new leads that can enrich both fields.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational neuroscience,Computer science,Visual system},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/MWM3ETJ5/Lotter et al. - 2020 - A neural network trained for prediction mimics diverse features of biological neurons and perception.pdf}
}

@article{Lund1993_148,
  title = {Comparison of {{Intrinsic Connectivity}} in {{Different Areas}} of {{Macaque Monkey Cerebral Cortex}}},
  author = {Lund, Jennifer S. and Yoshioka, Takashi and Levitt, Jonathan B.},
  year = {1993},
  month = mar,
  journal = {Cerebral Cortex},
  volume = {3},
  number = {2},
  pages = {148--162},
  issn = {1047-3211},
  doi = {10.1093/cercor/3.2.148},
  urldate = {2025-02-21},
  abstract = {We have used small injections of biocytin to label and compare patterns of intreareal, laterally spreading projections of pyramidal neurons in a number of areas of macaque monkey cerebral cortex. In visual areas (V1, V2, and V4), somatosensory areas (3b, 1, and 2), and motor area 4, a punctate discontinuous pattern of connections is made from 200-{$\mu$}m-diameter biocytin injections in the superficial layers. In prefrontal cortex (areas 9 and 46), stripe-like connectivity patterns are observed. In all areas of cortex examined, the width of the terminal-free gaps is closely scaled to the average diameter of terminal patches, or width of terminal stripes. In addition, both patch and gap dimensions match the average lateral spread of the dendritic field of single pyramidal neurons in the superficial layers of the same cortical region. These architectural features of the connectional mosaics are constant despite a twofold difference in scale across cortical areas and different species. They therefore appear to be fundamental features of cortical organization. A model is offered in which local circuit inhibitory ``basket'' interneurons, activated at the same time as excitatory pyramidal neurons, could veto pyramidal neuron connections within either circular or stripe-like domains; this could lead to the formation of the pattern of lateral connections observed in this study, and provides a framework for further theoretical studies of cerebral cortex function.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/QRQMD27N/Lund et al. - 1993 - Comparison of Intrinsic Connectivity in Different Areas of Macaque Monkey Cerebral Cortex.pdf}
}

@article{Ma2019_286,
  title = {Transformed {{L1}} Regularization for Learning Sparse Deep Neural Networks},
  author = {Ma, Rongrong and Miao, Jianyu and Niu, Lingfeng and Zhang, Peng},
  year = {2019},
  month = nov,
  journal = {Neural Networks},
  volume = {119},
  pages = {286--298},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.08.015},
  urldate = {2024-10-18},
  abstract = {Deep Neural Networks (DNNs) have achieved extraordinary success in numerous areas. However, DNNs often carry a large number of weight parameters, leading to the challenge of heavy memory and computation costs. Overfitting is another challenge for DNNs when the training data are insufficient. These challenges severely hinder the application of DNNs in resource-constrained platforms. In fact, many network weights are redundant and can be removed from the network without much loss of performance. In this paper, we introduce a new non-convex integrated transformed {$\ell$}1 regularizer to promote sparsity for DNNs, which removes redundant connections and unnecessary neurons simultaneously. Specifically, we apply the transformed {$\ell$}1 regularizer to the matrix space of network weights and utilize it to remove redundant connections. Besides, group sparsity is integrated to remove unnecessary neurons. An efficient stochastic proximal gradient algorithm is presented to solve the new model. To the best of our knowledge, this is the first work to develop a non-convex regularizer in sparse optimization based method to simultaneously promote connection-level and neuron-level sparsity for DNNs. Experiments on public datasets demonstrate the effectiveness of the proposed method.},
  keywords = {Deep neural networks,Group sparsity,Non-convex regularization,Transformed},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/RNH77D5C/S0893608019302321.html}
}

@inproceedings{Maniquet2024_,
  title = {Recurrent Issues with Deep Neural Networks of Visual Recognition},
  booktitle = {{{CCN}} 2024},
  author = {Maniquet, Tim and {Op de Beeck}, Hans and Ivan Costantino, Andrea},
  year = {2024},
  address = {Boston, MA},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/9QGX97RT/Maniquet - Recurrent issues with deep neural networks of visu.pdf}
}

@article{Mlynarski2022_e3001889,
  title = {Efficient Coding Theory of Dynamic Attentional Modulation},
  author = {M{\l}ynarski, Wiktor and Tka{\v c}ik, Ga{\v s}per},
  year = {2022},
  month = dec,
  journal = {PLOS Biology},
  volume = {20},
  number = {12},
  pages = {e3001889},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3001889},
  urldate = {2024-10-18},
  abstract = {Activity of sensory neurons is driven not only by external stimuli but also by feedback signals from higher brain areas. Attention is one particularly important internal signal whose presumed role is to modulate sensory representations such that they only encode information currently relevant to the organism at minimal cost. This hypothesis has, however, not yet been expressed in a normative computational framework. Here, by building on normative principles of probabilistic inference and efficient coding, we developed a model of dynamic population coding in the visual cortex. By continuously adapting the sensory code to changing demands of the perceptual observer, an attention-like modulation emerges. This modulation can dramatically reduce the amount of neural activity without deteriorating the accuracy of task-specific inferences. Our results suggest that a range of seemingly disparate cortical phenomena such as intrinsic gain modulation, attention-related tuning modulation, and response variability could be manifestations of the same underlying principles, which combine efficient sensory coding with optimal probabilistic inference in dynamic environments.},
  langid = {english},
  keywords = {Attention,Coding mechanisms,Neuronal tuning,Neurons,Sensory neurons,Sensory perception,Vision,Visual cortex},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/5JBPKED9/Mlynarski2022_e3001889.pdf}
}

@article{Molder2021_33,
  ids = {Molder2021_},
  title = {Sustainable Data Analysis with {{Snakemake}}},
  author = {M{\"o}lder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and {Tomkins-Tinch}, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and K{\"o}ster, Johannes},
  year = {2021},
  month = apr,
  journal = {F1000Research},
  volume = {10},
  pages = {33},
  issn = {2046-1402},
  doi = {10.12688/f1000research.29032.2},
  urldate = {2021-06-25},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {adaptability,data analysis,reproducibility,scalability,sustainability,transparency,workflow management},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/C7PVVST8/Molder2021_33.pdf;/home/rgutzen/Cloud/own/Zotero/storage/86TN23X8/v2.html}
}

@article{Nayebi_,
  title = {Task-{{Driven Convolutional Recurrent Models}} of the {{Visual System}}},
  author = {Nayebi, Aran and Bear, Daniel and Kubilius, Jonas and Kar, Kohitij and Ganguli, Surya and DiCarlo, James J and Yamins, Daniel L K},
  abstract = {Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classification tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain's visual system. However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas. Here we explored the role of recurrence in improving classification performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, novel cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identified novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs matched the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain's recurrent connections in performing difficult visual behaviors.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/TXEVGMHJ/Nayebi et al. - Task-Driven Convolutional Recurrent Models of the Visual System.pdf}
}

@article{Nayebi2022_1652,
  title = {Recurrent {{Connections}} in the {{Primate Ventral Visual Stream Mediate}} a {{Trade-Off Between Task Performance}} and {{Network Size During Core Object Recognition}}},
  author = {Nayebi, Aran and {Sagastuy-Brena}, Javier and Bear, Daniel M. and Kar, Kohitij and Kubilius, Jonas and Ganguli, Surya and Sussillo, David and DiCarlo, James J. and Yamins, Daniel L. K.},
  year = {2022},
  month = jul,
  journal = {Neural Computation},
  volume = {34},
  number = {8},
  pages = {1652--1675},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01506},
  urldate = {2024-11-08},
  abstract = {The computational role of the abundant feedback connections in the ventral visual stream is unclear, enabling humans and nonhuman primates to effortlessly recognize objects across a multitude of viewing conditions. Prior studies have augmented feedforward convolutional neural networks (CNNs) with recurrent connections to study their role in visual processing; however, often these recurrent networks are optimized directly on neural data or the comparative metrics used are undefined for standard feedforward networks that lack these connections. In this work, we develop task-optimized convolutional recurrent (ConvRNN) network models that more correctly mimic the timing and gross neuroanatomy of the ventral pathway. Properly chosen intermediate-depth ConvRNN circuit architectures, which incorporate mechanisms of feedforward bypassing and recurrent gating, can achieve high performance on a core recognition task, comparable to that of much deeper feedforward networks. We then develop methods that allow us to compare both CNNs and ConvRNNs to finely grained measurements of primate categorization behavior and neural response trajectories across thousands of stimuli. We find that high-performing ConvRNNs provide a better match to these data than feedforward networks of any depth, predicting the precise timings at which each stimulus is behaviorally decoded from neural activation patterns. Moreover, these ConvRNN circuits consistently produce quantitatively accurate predictions of neural dynamics from V4 and IT across the entire stimulus presentation. In fact, we find that the highest-performing ConvRNNs, which best match neural and behavioral data, also achieve a strong Pareto trade-off between task performance and overall network size. Taken together, our results suggest the functional purpose of recurrence in the ventral pathway is to fit a high-performing network in cortex, attaining computational power through temporal rather than spatial complexity.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/BU259IIE/Nayebi2022_1652.pdf;/home/rgutzen/Cloud/own/Zotero/storage/Q8RMMSIW/Recurrent-Connections-in-the-Primate-Ventral.html}
}

@article{Ohki2006_925,
  title = {Highly Ordered Arrangement of Single Neurons in Orientation Pinwheels},
  author = {Ohki, Kenichi and Chung, Sooyoung and Kara, Prakash and H{\"u}bener, Mark and Bonhoeffer, Tobias and Reid, R. Clay},
  year = {2006},
  month = aug,
  journal = {Nature},
  volume = {442},
  number = {7105},
  pages = {925--928},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature05019},
  urldate = {2025-02-21},
  abstract = {In the cat visual cortex, orientation-selective neurons are arranged in characteristic patterns known as 'pinwheels'. In vivo, two-photon calcium imaging reveals the highly ordered arrangement at the level of single cells for this functional architecture --- neurons selective to different orientations are strictly segregated, even at pinwheel centres.},
  copyright = {2006 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/WZULF5BJ/Ohki et al. - 2006 - Highly ordered arrangement of single neurons in orientation pinwheels.pdf}
}

@article{Paszke2017_,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  month = oct,
  urldate = {2025-02-21},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch --- a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/72BUZQY7/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf}
}

@article{Pogoncheff2023_13908,
  ids = {pogoncheff2023explaining},
  title = {Explaining {{V1 Properties}} with a {{Biologically Constrained Deep Learning Architecture}}},
  author = {Pogoncheff, Galen and Granley, Jacob and Beyeler, Michael},
  year = {2023},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {13908--13930},
  abstract = {Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neurosciencederived architectural components into CNNs to identify a set of mechanisms and architectures that more comprehensively explain V1 activity. Upon enhancing task-driven CNNs with architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Moreover, analyses of the learned parameters of these components and stimuli that maximally activate neurons of the evaluated networks provide support for their role in explaining neural properties of V1. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/EK7S7SHP/Pogoncheff et al. - Explaining V1 Properties with a Biologically Const.pdf}
}

@misc{Qian2024_2024.08.06.606687,
  title = {Local Lateral Connectivity Is Sufficient for Replicating Cortex-like Topographical Organization in Deep Neural Networks},
  author = {Qian, Xinyu and Dehghani, Amir Ozhan and Farahani, Asa Borzabadi and Bashivan, Pouya},
  year = {2024},
  month = aug,
  primaryclass = {New Results},
  pages = {2024.08.06.606687},
  publisher = {bioRxiv},
  doi = {10.1101/2024.08.06.606687},
  urldate = {2024-08-12},
  abstract = {Across the primate cortex, neurons that perform similar functions tend to be spatially grouped together. In the high-level visual cortex, this biological principle manifests itself as a modular organization of neuronal clusters, each tuned to a specific object category. The tendency toward short connections is widely believed to explain the existence of such an organization in the brains of many animals. However, the neural mechanisms underlying this phenomenon remain unclear. Here, we use artificial deep neural networks as test beds to demonstrate that a topographical organization akin to that in the primary, intermediate, and high-level human visual cortex emerges when units in these models are locally laterally connected and their weight parameters are tuned by top-down credit assignment (i.e. backpropagation of error). The emergence of modular organization without explicit topography-inducing learning rules or objective functions challenges their necessity and suggests that local lateral connectivity alone may suffice for the formation of topographic organization across the cortex. Furthermore, the incorporation of lateral connections in deep convolutional networks enhances their robustness to small input perturbations, indicating an additional role for these connections in learning robust representations.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/WMID9J9Y/Qian2024_2024.08.06.606687.pdf}
}

@article{Sanzeni2020_e54875,
  title = {Inhibition Stabilization Is a Widespread Property of Cortical Networks},
  author = {Sanzeni, Alessandro and Akitake, Bradley and Goldbach, Hannah C and Leedy, Caitlin E and Brunel, Nicolas and Histed, Mark H},
  editor = {O'Leary, Timothy and Huguenard, John and Adesnik, Hillel},
  year = {2020},
  month = jun,
  journal = {eLife},
  volume = {9},
  pages = {e54875},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.54875},
  urldate = {2025-02-21},
  abstract = {Many cortical network models use recurrent coupling strong enough to require inhibition for stabilization. Yet it has been experimentally unclear whether inhibition-stabilized network (ISN) models describe cortical function well across areas and states. Here, we test several ISN predictions, including the counterintuitive (paradoxical) suppression of inhibitory firing in response to optogenetic inhibitory stimulation. We find clear evidence for ISN operation in mouse visual, somatosensory, and motor cortex. Simple two-population ISN models describe the data well and let us quantify coupling strength. Although some models predict a non-ISN to ISN transition with increasingly strong sensory stimuli, we find ISN effects without sensory stimulation and even during light anesthesia. Additionally, average paradoxical effects result only with transgenic, not viral, opsin expression in padvmlbumin (PV)-positive neurons; theory and expression data show this is consistent with ISN operation. Taken together, these results show strong coupling and inhibition stabilization are common features of the cortex.},
  keywords = {cortical models,inhibitory stabilized network,model inference,optogenetics,paradoxical,transgenic animals},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/X6V8ZAB4/Sanzeni et al. - 2020 - Inhibition stabilization is a widespread property of cortical networks.pdf}
}

@misc{Schrimpf2020_407007,
  title = {Brain-{{Score}}: {{Which Artificial Neural Network}} for {{Object Recognition}} Is Most {{Brain-Like}}?},
  shorttitle = {Brain-{{Score}}},
  author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and {Prescott-Roy}, Jonathan and Geiger, Franziska and Schmidt, Kailyn and Yamins, Daniel L. K. and DiCarlo, James J.},
  year = {2020},
  month = jan,
  primaryclass = {New Results},
  pages = {407007},
  publisher = {bioRxiv},
  doi = {10.1101/407007},
  urldate = {2024-04-04},
  abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed Brain-Score -- a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain's mechanisms for core object recognition -- and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. (2) There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at {$\geq$} 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain's network and thus drive next experiments. To facilitate both of these, we release Brain-Score.org: a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/IUWGBRGC/Schrimpf2020_407007.pdf}
}

@article{Semedo2022_1099,
  title = {Feedforward and Feedback Interactions between Visual Cortical Areas Use Different Population Activity Patterns},
  author = {Semedo, Jo{\~a}o D. and Jasper, Anna I. and Zandvakili, Amin and Krishna, Aravind and Aschner, Amir and Machens, Christian K. and Kohn, Adam and Yu, Byron M.},
  year = {2022},
  month = mar,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {1099},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-28552-w},
  urldate = {2024-11-08},
  abstract = {Brain function relies on the coordination of activity across multiple, recurrently connected brain areas. For instance, sensory information encoded in early sensory areas is relayed to, and further processed by, higher cortical areas and then fed back. However, the way in which feedforward and feedback signaling interact with one another is incompletely understood. Here we investigate this question by leveraging simultaneous neuronal population recordings in early and midlevel visual areas (V1--V2 and V1--V4). Using a dimensionality reduction approach, we find that population interactions are feedforward-dominated shortly after stimulus onset and feedback-dominated during spontaneous activity. The population activity patterns most correlated across areas were distinct during feedforward- and feedback-dominated periods. These results suggest that feedforward and feedback signaling rely on separate ``channels'', which allows feedback signals to not directly affect activity that is fed forward.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Visual system},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/8387ASNU/Semedo2022_1099.pdf}
}

@article{Shi2018_2269,
  title = {Deep Recurrent Neural Network Reveals a Hierarchy of Process Memory during Dynamic Natural Vision},
  author = {Shi, Junxing and Wen, Haiguang and Zhang, Yizhen and Han, Kuan and Liu, Zhongming},
  year = {2018},
  month = feb,
  journal = {Human Brain Mapping},
  volume = {39},
  number = {5},
  pages = {2269--2282},
  issn = {1065-9471},
  doi = {10.1002/hbm.24006},
  urldate = {2025-02-18},
  abstract = {The human visual cortex extracts both spatial and temporal visual features to support perception and guide behavior. Deep convolutional neural networks (CNNs) provide a computational framework to model cortical representation and organization for spatial visual processing, but unable to explain how the brain processes temporal information. To overcome this limitation, we extended a CNN by adding recurrent connections to different layers of the CNN to allow spatial representations to be remembered and accumulated over time. The extended model, or the recurrent neural network (RNN), embodied a hierarchical and distributed model of process memory as an integral part of visual processing. Unlike the CNN, the RNN learned spatiotemporal features from videos to enable action recognition. The RNN better predicted cortical responses to natural movie stimuli than the CNN, at all visual areas, especially those along the dorsal stream. As a fully obsedvmble model of visual processing, the RNN also revealed a cortical hierarchy of temporal receptive window, dynamics of process memory, and spatiotemporal representations. These results support the hypothesis of process memory, and demonstrate the potential of using the RNN for in-depth computational understanding of dynamic natural vision.},
  pmcid = {PMC5895512},
  pmid = {29436055},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/5XHVKNPU/Shi et al. - 2018 - Deep recurrent neural network reveals a hierarchy of process memory during dynamic natural vision.pdf}
}

@article{Soo2023_2023.10.10.561588,
  title = {Training Biologically Plausible Recurrent Neural Networks on Cognitive Tasks with Long-Term Dependencies},
  author = {Soo, Wayne W. and Goudar, Vishwa and Wang, Xiao-Jing},
  year = {2023},
  month = oct,
  journal = {bioRxiv},
  pages = {2023.10.10.561588},
  doi = {10.1101/2023.10.10.561588},
  urldate = {2025-02-21},
  abstract = {Training recurrent neural networks (RNNs) has become a go-to approach for generating and evaluating mechanistic neural hypotheses for cognition. The ease and efficiency of training RNNs with backpropagation through time and the availability of robustly supported deep learning libraries has made RNN modeling more approachable and accessible to neuroscience. Yet, a major technical hindrance remains. Cognitive processes such as working memory and decision making involve neural population dynamics over a long period of time within a behavioral trial and across trials. It is difficult to train RNNs to accomplish tasks where neural representations and dynamics have long temporal dependencies without gating mechanisms such as LSTMs or GRUs which currently lack experimental support and prohibit direct comparison between RNNs and biological neural circuits. We tackled this problem based on the idea of specialized skip-connections through time to support the emergence of task-relevant dynamics, and subsequently reinstitute biological plausibility by reverting to the original architecture. We show that this approach enables RNNs to successfully learn cognitive tasks that prove impractical if not impossible to learn using conventional methods. Over numerous tasks considered here, we achieve less training steps and shorter wall-clock times, particularly in tasks that require learning long-term dependencies via temporal integration over long timescales or maintaining a memory of past events in hidden-states. Our methods expand the range of experimental tasks that biologically plausible RNN models can learn, thereby supporting the development of theory for the emergent neural mechanisms of computations involving long-term dependencies.},
  pmcid = {PMC10592728},
  pmid = {37873445},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/8ERUSA6Q/Soo et al. - 2023 - Training biologically plausible recurrent neural networks on cognitive tasks with long-term dependen.pdf}
}

@article{Soo2023_32061,
  title = {Training Biologically Plausible Recurrent Neural Networks on Cognitive Tasks with Long-Term Dependencies},
  author = {Soo, Wayne and Goudar, Vishwa and Wang, Xiao-Jing},
  year = {2023},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {32061--32074},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/L9TMCJST/Soo et al. - 2023 - Training biologically plausible recurrent neural networks on cognitive tasks with long-term dependen.pdf}
}

@inproceedings{Soo2024_,
  title = {Recurrent Neural Network Dynamical Systems for Biological Vision},
  booktitle = {The {{Thirty-eighth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Soo, Wayne W. and Battista, Aldo and Radmard, Puria and Wang, Xiao-Jing},
  year = {2024},
  month = nov,
  urldate = {2024-11-11},
  abstract = {In neuroscience, recurrent neural networks (RNNs) are modeled as continuous-time dynamical systems to more accurately reflect the dynamics inherent in biological circuits. However, convolutional neural networks (CNNs) remain the preferred architecture in vision neuroscience due to their ability to efficiently process visual information, which comes at the cost of the biological realism provided by RNNs. To address this, we introduce a hybrid architecture that integrates the continuous-time recurrent dynamics of RNNs with the spatial processing capabilities of CNNs. Our models preserve the dynamical characteristics typical of RNNs while having comparable performance with their conventional CNN counterparts on benchmarks like ImageNet. Compared to conventional CNNs, our models demonstrate increased robustness to noise due to noise-suppressing mechanisms inherent in recurrent dynamical systems. Analyzing our architecture as a dynamical system is computationally expensive, so we develop a toolkit consisting of iterative methods specifically tailored for convolutional structures. We also train multi-area RNNs using our architecture as the front-end to perform complex cognitive tasks previously impossible to learn or achievable only with oversimplified stimulus representations. In monkey neural recordings, our models capture time-dependent variations in neural activity in higher-order visual areas. Together, these contributions represent a comprehensive foundation to unify the advances of CNNs and dynamical RNNs in vision neuroscience.},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/HPBH8Y2T/Soo2024_.pdf}
}

@article{Spoerer2017_,
  title = {Recurrent {{Convolutional Neural Networks}}: {{A Better Model}} of {{Biological Object Recognition}}},
  shorttitle = {Recurrent {{Convolutional Neural Networks}}},
  author = {Spoerer, Courtney J. and McClure, Patrick and Kriegeskorte, Nikolaus},
  year = {2017},
  month = sep,
  journal = {Frontiers in Psychology},
  volume = {8},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.01551},
  urldate = {2024-10-18},
  abstract = {{$<$}p{$>$}Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and non-human primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efficacy of the models, {$<$}italic{$>$}digit clutter{$<$}/italic{$>$} (where multiple target digits occlude one another) and {$<$}italic{$>$}digit debris{$<$}/italic{$>$} (where target digits are occluded by digit fragments). We find that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognizing objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognize objects, especially under challenging conditions. This work shows that computer vision can benefit from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Convolutional Neural Network,object recognition,occlusion,recurrent neural network,top-down processing},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/WV45SKCQ/Spoerer2017_.pdf}
}

@article{Spoerer2020_e1008215,
  title = {Recurrent Neural Networks Can Explain Flexible Trading of Speed and Accuracy in Biological Vision},
  author = {Spoerer, Courtney J. and Kietzmann, Tim C. and Mehrer, Johannes and Charest, Ian and Kriegeskorte, Nikolaus},
  year = {2020},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {10},
  pages = {e1008215},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008215},
  urldate = {2023-11-14},
  abstract = {Deep feedforward neural network models of vision dominate in both computational neuroscience and engineering. The primate visual system, by contrast, contains abundant recurrent connections. Recurrent signal flow enables recycling of limited computational resources over time, and so might boost the performance of a physically finite brain or model. Here we show: (1) Recurrent convolutional neural network models outperform feedforward convolutional models matched in their number of parameters in large-scale visual recognition tasks on natural images. (2) Setting a confidence threshold, at which recurrent computations terminate and a decision is made, enables flexible trading of speed for accuracy. At a given confidence threshold, the model expends more time and energy on images that are harder to recognise, without requiring additional parameters for deeper computations. (3) The recurrent model's reaction time for an image predicts the human reaction time for the same image better than several parameter-matched and state-of-the-art feedforward models. (4) Across confidence thresholds, the recurrent model emulates the behaviour of feedforward control models in that it achieves the same accuracy at approximately the same computational cost (mean number of floating-point operations). However, the recurrent model can be run longer (higher confidence threshold) and then outperforms parameter-matched feedforward comparison models. These results suggest that recurrent connectivity, a hallmark of biological visual systems, may be essential for understanding the accuracy, flexibility, and dynamics of human visual recognition.},
  langid = {english},
  keywords = {Entropy,Feedforward neural networks,Graphs,Principal component analysis,Reaction time,Recurrent neural networks,Visual object recognition,Visual system},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/GVPE4NBQ/Spoerer2020_e1008215.pdf}
}

@inproceedings{Stefanidi2023_,
  title = {Pretraining on Long-Timescale Autonomous Neural Dynamics Supports Task Learning in Artificial {{RNNs}}},
  booktitle = {Bernstein {{Conference}} 2023},
  author = {Stefanidi, Zinovia and Gao, Richard and Pals, Matthijs and Macke, Jakob H.},
  year = {2023},
  address = {Berlin},
  doi = {10.12751/nncn.bc2023.043},
  urldate = {2024-11-24},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/LB8ATSAS/abstracts.html}
}

@article{Stettler2002_739,
  title = {Lateral {{Connectivity}} and {{Contextual Interactions}} in {{Macaque Primary Visual Cortex}}},
  author = {Stettler, Dan D. and Das, Aniruddha and Bennett, Jean and Gilbert, Charles D.},
  year = {2002},
  month = nov,
  journal = {Neuron},
  volume = {36},
  number = {4},
  pages = {739--750},
  publisher = {Elsevier},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(02)01029-2},
  urldate = {2025-02-21},
  langid = {english},
  pmid = {12441061},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/JZAPQGMA/Stettler et al. - 2002 - Lateral Connectivity and Contextual Interactions in Macaque Primary Visual Cortex.pdf}
}

@article{Stewart2020_2,
  title = {A Review of Interactions between Peripheral and Foveal Vision},
  author = {Stewart, Emma E. M. and Valsecchi, Matteo and Sch{\"u}tz, Alexander C.},
  year = {2020},
  month = nov,
  journal = {Journal of Vision},
  volume = {20},
  number = {12},
  pages = {2},
  issn = {1534-7362},
  doi = {10.1167/jov.20.12.2},
  urldate = {2024-10-19},
  abstract = {Visual processing varies dramatically across the visual field. These differences start in the retina and continue all the way to the visual cortex. Despite these differences in processing, the perceptual experience of humans is remarkably stable and continuous across the visual field. Research in the last decade has shown that processing in peripheral and foveal vision is not independent, but is more directly connected than previously thought. We address three core questions on how peripheral and foveal vision interact, and review recent findings on potentially related phenomena that could provide answers to these questions. First, how is the processing of peripheral and foveal signals related during fixation? Peripheral signals seem to be processed in foveal retinotopic areas to facilitate peripheral object recognition, and foveal information seems to be extrapolated toward the periphery to generate a homogeneous representation of the environment. Second, how are peripheral and foveal signals re-calibrated? Transsaccadic changes in object features lead to a reduction in the discrepancy between peripheral and foveal appearance. Third, how is peripheral and foveal information stitched together across saccades? Peripheral and foveal signals are integrated across saccadic eye movements to average percepts and to reduce uncertainty. Together, these findings illustrate that peripheral and foveal processing are closely connected, mastering the compromise between a large peripheral visual field and high resolution at the fovea.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/VHBGBMVJ/Stewart2020_2.pdf;/home/rgutzen/Cloud/own/Zotero/storage/8PPUJMAX/article.html}
}

@misc{Tallec2018_,
  title = {Can Recurrent Neural Networks Warp Time?},
  author = {Tallec, Corentin and Ollivier, Yann},
  year = {2018},
  month = mar,
  number = {arXiv:1804.11188},
  eprint = {1804.11188},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.11188},
  urldate = {2024-11-24},
  abstract = {Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use ad hoc gating mechanisms. Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues. We prove that learnable gates in a recurrent model formally provide quasi- invariance to general time transformations in the input data. We recover part of the LSTM architecture from a simple axiomatic approach. This result leads to a new way of initializing gate biases in LSTMs and GRUs. Ex- perimentally, this new chrono initialization is shown to greatly improve learning of long term dependencies, with minimal implementation effort.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/GIMUVNAG/Tallec and Ollivier - 2018 - Can recurrent neural networks warp time.pdf;/home/rgutzen/Cloud/own/Zotero/storage/YC8LTU7T/1804.html}
}

@article{Tang2018_8835,
  title = {Recurrent Computations for Visual Pattern Completion},
  author = {Tang, Hanlin and Schrimpf, Martin and Lotter, William and Moerman, Charlotte and Paredes, Ana and Ortega Caro, Josue and Hardesty, Walter and Cox, David and Kreiman, Gabriel},
  year = {2018},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {35},
  pages = {8835--8840},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1719397115},
  urldate = {2023-12-04},
  abstract = {Making inferences from partial information constitutes a critical aspect of cognition. During visual perception, pattern completion enables recognition of poorly visible or occluded objects. We combined psychophysics, physiology, and computational models to test the hypothesis that pattern completion is implemented by recurrent computations and present three pieces of evidence that are consistent with this hypothesis. First, subjects robustly recognized objects even when they were rendered {$<$}15\% visible, but recognition was largely impaired when processing was interrupted by backward masking. Second, invasive physiological responses along the human ventral cortex exhibited visually selective responses to partially visible objects that were delayed compared with whole objects, suggesting the need for additional computations. These physiological delays were correlated with the effects of backward masking. Third, state-of-the-art feed-forward computational architectures were not robust to partial visibility. However, recognition performance was recovered when the model was augmented with attractor-based recurrent connectivity. The recurrent model was able to predict which images of heavily occluded objects were easier or harder for humans to recognize, could capture the effect of introducing a backward mask on recognition behavior, and was consistent with the physiological delays along the human ventral visual stream. These results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/NUUQCMIQ/Tang2018_8835.pdf}
}

@misc{Thepandasdevelopmentteam2022_,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  shorttitle = {Pandas-Dev/Pandas},
  author = {{The pandas development team}},
  year = {2022},
  month = sep,
  doi = {10.5281/zenodo.7093122},
  urldate = {2022-10-04},
  abstract = {This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version. See the full whatsnew for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher. The release will be available on the defaults and conda-forge channels: conda install -c conda-forge pandas Or via PyPI: python3 -m pip install --upgrade pandas Please report any issues with the release on the pandas issue tracker.},
  howpublished = {Zenodo},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/LNIT6NVG/7093122.html}
}

@misc{Thorat2022_,
  title = {Category-Orthogonal Object Features Guide Information Processing in Recurrent Neural Networks Trained for Object Categorization},
  author = {Thorat, Sushrut and Aldegheri, Giacomo and Kietzmann, Tim C.},
  year = {2022},
  month = may,
  number = {arXiv:2111.07898},
  eprint = {2111.07898},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.07898},
  urldate = {2025-02-21},
  abstract = {Recurrent neural networks (RNNs) have been shown to perform better than feedforward architectures in visual object categorization tasks, especially in challenging conditions such as cluttered images. However, little is known about the exact computational role of recurrent information flow in these conditions. Here we test RNNs trained for object categorization on the hypothesis that recurrence iteratively aids object categorization via the communication of category-orthogonal auxiliary variables (the location, orientation, and scale of the object). Using diagnostic linear readouts, we find that: (a) information about auxiliary variables increases across time in all network layers, (b) this information is indeed present in the recurrent information flow, and (c) its manipulation significantly affects task performance. These obsedvmtions confirm the hypothesis that category-orthogonal auxiliary variable information is conveyed through recurrent connectivity and is used to optimize category inference in cluttered environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/78LY8Y9J/Thorat et al. - 2022 - Category-orthogonal object features guide information processing in recurrent neural networks traine.pdf}
}

@article{Turrigiano2008_422,
  title = {The {{Self-Tuning Neuron}}: {{Synaptic Scaling}} of {{Excitatory Synapses}}},
  shorttitle = {The {{Self-Tuning Neuron}}},
  author = {Turrigiano, Gina G.},
  year = {2008},
  month = oct,
  journal = {Cell},
  volume = {135},
  number = {3},
  pages = {422--435},
  publisher = {Elsevier},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2008.10.008},
  urldate = {2025-02-19},
  langid = {english},
  pmid = {18984155},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/FN7H6X5C/Turrigiano - 2008 - The Self-Tuning Neuron Synaptic Scaling of Excitatory Synapses.pdf}
}

@article{vanBergen2020_176,
  title = {Going in Circles Is the Way Forward: The Role of Recurrence in Visual Inference},
  shorttitle = {Going in Circles Is the Way Forward},
  author = {{van Bergen}, Ruben S and Kriegeskorte, Nikolaus},
  year = {2020},
  month = dec,
  journal = {Current Opinion in Neurobiology},
  series = {Whole-Brain Interactions between Neural Circuits},
  volume = {65},
  pages = {176--193},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2020.11.009},
  urldate = {2024-10-17},
  abstract = {Biological visual systems exhibit abundant recurrent connectivity. State-of-the-art neural network models for visual recognition, by contrast, rely heavily or exclusively on feedforward computation. Any finite-time recurrent neural network (RNN) can be unrolled along time to yield an equivalent feedforward neural network (FNN). This important insight suggests that computational neuroscientists may not need to engage recurrent computation, and that computer-vision engineers may be limiting themselves to a special case of FNN if they build recurrent models. Here we argue, to the contrary, that FNNs are a special case of RNNs and that computational neuroscientists and engineers should engage recurrence to understand how brains and machines can (1) achieve greater and more flexible computational depth (2) compress complex computations into limited hardware (3) integrate priors and priorities into visual inference through expectation and attention (4) exploit sequential dependencies in their data for better inference and prediction and (5) leverage the power of iterative computation.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/2D8PTUSU/vanBergen2020_176.pdf;/home/rgutzen/Cloud/own/Zotero/storage/YCHKLMVP/S0959438820301768.html}
}

@article{Wang2020_117,
  title = {Recurrent Convolutional Neural Network: {{A}} New Framework for Remaining Useful Life Prediction of Machinery},
  shorttitle = {Recurrent Convolutional Neural Network},
  author = {Wang, Biao and Lei, Yaguo and Yan, Tao and Li, Naipeng and Guo, Liang},
  year = {2020},
  month = feb,
  journal = {Neurocomputing},
  volume = {379},
  pages = {117--129},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.10.064},
  urldate = {2025-02-21},
  abstract = {Deep learning is becoming more appealing in remaining useful life (RUL) prediction of machines, because it is able to automatically build the mapping relationship between the raw data and the corresponding RUL by representation learning. Among deep learning models, convolutional neural networks (CNNs) are gaining special attention because of its powerful ability in dealing with time-series signals, and have achieved promising results in current studies. These studies, however, suffer from the two limitations: (1) The temporal dependencies of different degradation states are not considered during network construction; and (2) The uncertainty of RUL prediction results cannot be obtained. To overcome the above-mentioned limitations, a new framework named recurrent convolutional neural network (RCNN) is proposed in this paper for RUL prediction of machinery. In RCNN, recurrent convolutional layers are first constructed to model the temporal dependencies of different degradation states. Then, variational inference is used to quantify the uncertainty of RCNN in RUL prediction. The proposed RCNN is evaluated using vibration data from accelerated degradation tests of rolling element bearings and sensor data from life testing of milling cutters, and compared with some state-of-the-art prognostics approaches. Experimental results demonstrate the effectiveness and superiority of RCNN in improving the accuracy and convergence of RUL prediction. More importantly, RCNN is able to provide a probabilistic RUL prediction result, which breaks the inherent limitation of CNNs and facilitates maintenance decision making.},
  keywords = {Convolutional neural network,Deep learning,Recurrent connection,Remaining useful life prediction,Uncertainty quantification},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/B4C2RZLM/S0925231219315024.html}
}

@article{Wang2021_1,
  title = {Convolutional {{Neural Networks}} with {{Gated Recurrent Connections}}},
  author = {Wang, Jianfeng and Hu, Xiaolin},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2106.02859},
  primaryclass = {cs},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3054614},
  urldate = {2025-02-18},
  abstract = {The convolutional neural network (CNN) has become a basic model for solving many computer vision problems. In recent years, a new class of CNNs, recurrent convolution neural network (RCNN), inspired by abundant recurrent connections in the visual systems of animals, was proposed. The critical element of RCNN is the recurrent convolutional layer (RCL), which incorporates recurrent connections between neurons in the standard convolutional layer. With increasing number of recurrent computations, the receptive fields (RFs) of neurons in RCL expand unboundedly, which is inconsistent with biological facts. We propose to modulate the RFs of neurons by introducing gates to the recurrent connections. The gates control the amount of context information inputting to the neurons and the neurons' RFs therefore become adaptive. The resulting layer is called gated recurrent convolution layer (GRCL).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/97G6V4R4/Wang and Hu - 2021 - Convolutional Neural Networks with Gated Recurrent Connections.pdf}
}

@misc{Wang2024_,
  title = {Efficient {{Hyperparameter Importance Assessment}} for {{CNNs}}},
  author = {Wang, Ruinan and Nabney, Ian and Golbabaee, Mohammad},
  year = {2024},
  month = oct,
  number = {arXiv:2410.08920},
  eprint = {2410.08920},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.08920},
  urldate = {2025-01-03},
  abstract = {Hyperparameter selection is an essential aspect of the machine learning pipeline, profoundly impacting models' robustness, stability, and generalization capabilities. Given the complex hyperparameter spaces associated with Neural Networks and the constraints of computational resources and time, optimizing all hyperparameters becomes impractical. In this context, leveraging hyperparameter importance assessment (HIA) can provide valuable guidance by narrowing down the search space. This enables machine learning practitioners to focus their optimization efforts on the hyperparameters with the most significant impact on model performance while conserving time and resources. This paper aims to quantify the importance weights of some hyperparameters in Convolutional Neural Networks (CNNs) with an algorithm called NRReliefF, laying the groundwork for applying HIA methodologies in the Deep Learning field. We conduct an extensive study by training over ten thousand CNN models across ten popular image classification datasets, thereby acquiring a comprehensive dataset containing hyperparameter configuration instances and their corresponding performance metrics. It is demonstrated that among the investigated hyperparameters, the top five important hyperparameters of the CNN model are the number of convolutional layers, learning rate, dropout rate, optimizer and epoch.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/MD9YXI4C/Wang et al. - 2024 - Efficient Hyperparameter Importance Assessment for CNNs.pdf}
}

@article{Wyatte2014_,
  title = {Early Recurrent Feedback Facilitates Visual Object Recognition under Challenging Conditions},
  author = {Wyatte, Dean and Jilk, David J. and O'Reilly, Randall C.},
  year = {2014},
  journal = {Frontiers in Psychology},
  volume = {5},
  issn = {1664-1078},
  urldate = {2023-12-04},
  abstract = {Standard models of the visual object recognition pathway hold that a largely feedforward process from the retina through inferotemporal cortex leads to object identification. A subsequent feedback process originating in frontoparietal areas through reciprocal connections to striate cortex provides attentional support to salient or behaviorally-relevant features. Here, we review mounting evidence that feedback signals also originate within extrastriate regions and begin during the initial feedforward process. This feedback process is temporally dissociable from attention and provides important functions such as grouping, associational reinforcement, and filling-in of features. Local feedback signals operating concurrently with feedforward processing are important for object identification in noisy real-world situations, particularly when objects are partially occluded, unclear, or otherwise ambiguous. Altogether, the dissociation of early and late feedback processes presented here expands on current models of object identification, and suggests a dual role for descending feedback projections.},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/IVXUKQ94/Wyatte2014_.pdf}
}

@article{Yu2025_92101,
  title = {Second-Order Forward-Mode Optimization of Recurrent Neural Networks for Neuroscience},
  author = {Yu, Youjing and Xia, Rui and Ma, Qingxi and Lengyel, Mate and Hennequin, Guillaume},
  year = {2025},
  month = jan,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {92101--92129},
  urldate = {2025-02-21},
  langid = {english},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/3WE6HANT/Yu et al. - 2025 - Second-order forward-mode optimization of recurrent neural networks for neuroscience.pdf}
}

@misc{Zeraati2024_,
  title = {Neural Timescales from a Computational Perspective},
  author = {Zeraati, Roxana and Levina, Anna and Macke, Jakob H. and Gao, Richard},
  year = {2024},
  month = sep,
  number = {arXiv:2409.02684},
  eprint = {2409.02684},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.02684},
  urldate = {2024-11-22},
  abstract = {Timescales of neural activity are diverse across and within brain areas, and experimental obsedvmtions suggest that neural timescales reflect information in dynamic environments. However, these obsedvmtions do not specify how neural timescales are shaped, nor whether particular timescales are necessary for neural computations and brain function. Here, we take a complementary perspective and synthesize three directions where computational methods can distill the broad set of empirical obsedvmtions into quantitative and testable theories: We review (i) how data analysis methods allow us to capture different timescales of neural dynamics across different recording modalities, (ii) how computational models provide a mechanistic explanation for the emergence of diverse timescales, and (iii) how task-optimized models in machine learning uncover the functional relevance of neural timescales. This integrative computational approach, combined with empirical findings, would provide a more holistic understanding of how neural timescales capture the relationship between brain structure, dynamics, and behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/SYD5QE6L/Zeraati et al. - 2024 - Neural timescales from a computational perspective.pdf;/home/rgutzen/Cloud/own/Zotero/storage/4JCHBHW3/2409.html}
}

@article{Zhang2024_,
  title = {Composing Recurrent Spiking Neural Networks Using Locally-Recurrent Motifs and Risk-Mitigating Architectural Optimization},
  author = {Zhang, Wenrui and Geng, Hejia and Li, Peng},
  year = {2024},
  month = jun,
  journal = {Frontiers in Neuroscience},
  volume = {18},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2024.1412559},
  urldate = {2025-02-18},
  abstract = {{$<$}p{$>$}In neural circuits, recurrent connectivity plays a crucial role in network function and stability. However, existing recurrent spiking neural networks (RSNNs) are often constructed by random connections without optimization. While RSNNs can produce rich dynamics that are critical for memory formation and learning, systemic architectural optimization of RSNNs is still an open challenge. We aim to enable systematic design of large RSNNs via a new scalable RSNN architecture and automated architectural optimization. We compose RSNNs based on a layer architecture called Sparsely-Connected Recurrent Motif Layer (SC-ML) that consists of multiple small recurrent motifs wired together by sparse lateral connections. The small size of the motifs and sparse inter-motif connectivity leads to an RSNN architecture scalable to large network sizes. We further propose a method called Hybrid Risk-Mitigating Architectural Search (HRMAS) to systematically optimize the topology of the proposed recurrent motifs and SC-ML layer architecture. HRMAS is an alternating two-step optimization process by which we mitigate the risk of network instability and performance degradation caused by architectural change by introducing a novel biologically-inspired ``self-repairing'' mechanism through intrinsic plasticity. The intrinsic plasticity is introduced to the second step of each HRMAS iteration and acts as unsupervised fast self-adaptation to structural and synaptic weight modifications introduced by the first step during the RSNN architectural ``evolution.'' We demonstrate that the proposed automatic architecture optimization leads to significant performance gains over existing manually designed RSNNs: we achieve 96.44\% on TI46-Alpha, 94.66\% on N-TIDIGITS, 90.28\% on DVS-Gesture, and 98.72\% on N-MNIST. To the best of the authors' knowledge, this is the first work to perform systematic architecture optimization on RSNNs.{$<$}/p{$>$}},
  langid = {english},
  keywords = {brain inspired computing,Intrinsic Plasticity,Neural architecture search,recurrent spiking neural networks,Sparsely-Connected Recurrent Motif Layer},
  file = {/home/rgutzen/Cloud/own/Zotero/storage/CE4ASEKC/Zhang et al. - 2024 - Composing recurrent spiking neural networks using locally-recurrent motifs and risk-mitigating archi.pdf}
}
