from collections import deque
from typing import Callable, Optional, Union, Deque, Dict, Any

import torch
import torch.nn as nn
from torchsummary import summary

from dynvision.model_components.topographic_recurrence import (
    LocalLateralConnection,
    LocalSeparableConnection,
)
from dynvision.utils import apply_parametrization
from dynvision.utils import str_to_bool
from pytorch_lightning import LightningModule
import logging


logger = logging.getLogger(__name__)

__all__ = [
    "DepthwiseSeparableConnection",
    "DepthPointwiseConnection",
    "PointDepthwiseConnection",
    "FullConnection",
    "SelfConnection",
    "InputAdaption",
    "RecurrentConnectedConv2d",
]


class RecurrenceBase(LightningModule):
    """
    Base class for recurrent connections providing common parameter initialization
    and argument validation.
    """

    def __init__(self, max_weight_init: float = 0.05, **kwargs) -> None:
        super().__init__()
        self.max_weight_init = max_weight_init
        self._parameters_initialized = False

    def validate_init_args(self, kwargs: Dict[str, Any]) -> None:
        """Validate required arguments for convolutional recurrence."""
        required = {"in_channels", "kernel_size"}
        missing = required - set(kwargs.keys())
        if missing:
            raise ValueError(f"Missing required arguments for convolution: {missing}")

    def _init_parameters(self) -> None:
        """Common parameter initialization for recurrent connections."""
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                if hasattr(module, "weight.original0"):
                    module.weight.original0.data.fill_(1.0)
                if hasattr(module, "weight.original1"):
                    nn.init.uniform_(
                        module.weight.original1.weight,
                        a=-self.max_weight_init,
                        b=self.max_weight_init,
                    )
                elif hasattr(module, "weight"):
                    nn.init.uniform_(
                        module.weight, a=-self.max_weight_init, b=self.max_weight_init
                    )
                if hasattr(module, "bias") and module.bias is not None:
                    nn.init.constant_(module.bias, 0)


class ConvolutionalRecurrenceBase(RecurrenceBase):
    """
    Base class for recurrence types that use convolutions.
    Provides common validation for convolution-specific parameters.
    """

    pass


class DepthwiseSeparableConnection(RecurrenceBase):
    """
    Implements a depthwise separable convolution connection.

    This connection type first applies a depthwise convolution (separate convolution
    for each channel) followed by a pointwise convolution (1x1 convolution across channels).
    This decomposition reduces parameter count while maintaining expressivity.
    """

    def __init__(
        self,
        in_channels: int,
        kernel_size: int,
        bias: bool = False,
        max_weight_init: float = 0.05,
        parametrization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
        **kwargs,
    ) -> None:
        """
        Initialize the DepthwiseSeparableConnection.

        Args:
            in_channels (int): Number of input channels.
            kernel_size (int): Size of the convolution kernel.
            bias (bool): Whether to include a bias term. Default is False.
            max_weight_init (float): Maximum weight initialization value. Default is 0.05.
            parametrization (Callable): Function to apply to the convolution layers. Default is identity.
        """
        super().__init__(max_weight_init=max_weight_init, **kwargs)

        # Store initialization arguments as attributes
        self.in_channels = in_channels
        self.kernel_size = kernel_size
        self.bias = bias
        self.parametrization = parametrization

        self._define_architecture()

    def _define_architecture(self) -> None:
        """Define the architecture of the depthwise separable connection."""
        self.depthwise_conv = nn.Conv2d(
            in_channels=self.in_channels,
            out_channels=self.in_channels,
            kernel_size=self.kernel_size,
            stride=1,
            padding=self.kernel_size // 2,
            groups=self.in_channels,
            bias=self.bias,
        )

        self.pointwise_conv = nn.Conv2d(
            in_channels=self.in_channels,
            out_channels=self.in_channels,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=self.bias,
        )

        if self.parametrization is not None:
            self.depthwise_conv = apply_parametrization(
                self.depthwise_conv, self.parametrization
            )
            self.pointwise_conv = apply_parametrization(
                self.pointwise_conv, self.parametrization
            )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the depthwise separable connection.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor.
        """
        h = self.depthwise_conv(x)
        h = self.pointwise_conv(h)
        return h


class DepthPointwiseConnection(DepthwiseSeparableConnection):
    """
    Implements a depth-pointwise convolution connection.
    """

    pass


class PointDepthwiseConnection(DepthwiseSeparableConnection):
    """
    Implements a point-depthwise convolution connection.
    """

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the point-depthwise connection.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor.
        """
        h = self.pointwise_conv(x)
        h = self.depthwise_conv(h)
        return h


class FullConnection(RecurrenceBase):
    """
    Implements a full convolution connection.

    This connection type applies a full convolutional operation where each unit
    receives input from all units within a nearby spatial region across all channels.
    This provides the most complete form of local connectivity but with the highest
    parameter count.
    """

    def __init__(
        self,
        in_channels: int,
        kernel_size: int,
        bias: bool = False,
        max_weight_init: float = 0.05,
        parametrization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
        **kwargs,
    ) -> None:
        """
        Initialize the FullConnection.

        Args:
            in_channels (int): Number of input channels.
            kernel_size (int): Size of the convolution kernel.
            bias (bool): Whether to include a bias term. Default is False.
            max_weight_init (float): Maximum weight initialization value. Default is 0.05.
            parametrization (Callable): Function to apply to the convolution layers. Default is identity.
        """
        super().__init__(max_weight_init=max_weight_init, **kwargs)

        # Store initialization arguments as attributes
        self.in_channels = in_channels
        self.kernel_size = kernel_size
        self.bias = bias
        self.parametrization = parametrization

        self._define_architecture()

    def _define_architecture(self) -> None:
        """Define the architecture of the full connection."""
        self.conv = nn.Conv2d(
            in_channels=self.in_channels,
            out_channels=self.in_channels,
            kernel_size=self.kernel_size,
            stride=1,
            padding=self.kernel_size // 2,
            bias=self.bias,
        )
        if self.parametrization is not None:
            self.conv = apply_parametrization(self.conv, self.parametrization)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the full connection.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor.
        """
        return self.conv(x)


class SelfConnection(RecurrenceBase):
    """
    Implements a self-connection with optional fixed weight and bias.

    This is the simplest form of recurrence where a unit connects only to itself.
    It can be used to model the persistence of neural activity over time or as a
    simplified form of neural adaptation.
    """

    def __init__(
        self,
        fixed_weight: Optional[float] = None,
        max_weight_init: float = 0.2,
        bias: bool = True,
        **kwargs,
    ) -> None:
        """
        Initialize the SelfConnection.

        Args:
            fixed_weight (Optional[float]): Fixed weight value. Default is None.
            max_weight_init (float): Maximum weight initialization value. Default is 0.2.
            bias (bool): Whether to include a bias term. Default is True.
        """
        super().__init__(max_weight_init=max_weight_init, **kwargs)

        # Store initialization arguments as attributes
        self.fixed_weight = fixed_weight
        self.bias_enabled = bias
        self.requires_grad = fixed_weight is None

        self._define_architecture()

    def _define_architecture(self) -> None:
        """Define the architecture of the self connection."""
        self.weight = nn.Parameter(torch.Tensor([1]), requires_grad=self.requires_grad)
        if self.bias_enabled:
            self.bias = nn.Parameter(torch.Tensor([0]), requires_grad=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the self-connection.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor.
        """
        if hasattr(self, "bias"):
            return x * self.weight + self.bias
        return x * self.weight

    def _init_parameters(self) -> None:
        """
        Initialize parameters for the self-connection.
        """
        if self.requires_grad:
            nn.init.uniform_(self.weight, a=-self.max_weight_init, b=0)
        else:
            self.weight.data.fill_(self.fixed_weight)
        if hasattr(self, "bias"):
            nn.init.constant_(self.bias, 0)


class InputAdaption(LightningModule):
    """
    Implements input adaptation with a self-connection.
    """

    def __init__(
        self,
        fixed_weight: Optional[float] = None,
        max_weight_init: float = 0.2,
        **kwargs,
    ) -> None:
        """
        Initialize the InputAdaption.

        Args:
            fixed_weight (Optional[float]): Fixed weight value. Default is None.
            max_weight_init (float): Maximum weight initialization value. Default is 0.2.
        """
        super(InputAdaption, self).__init__()

        self.recurrence = SelfConnection(
            fixed_weight=fixed_weight,
            max_weight_init=max_weight_init,
            bias=False,
            **kwargs,
        )
        self.reset()

    def reset(self) -> None:
        """
        Reset the hidden state.
        """
        self.hidden_state = None

    def forward(self, x: Optional[torch.Tensor] = None) -> Optional[torch.Tensor]:
        """
        Forward pass for the input adaptation.

        Args:
            x (Optional[torch.Tensor]): Input tensor.

        Returns:
            Optional[torch.Tensor]: Output tensor.
        """
        if self.hidden_state is None:
            self.hidden_state = x
            return x

        else:
            h = self.recurrence(self.hidden_state)
            self.hidden_state = h
            return h


class RecurrentConnectedConv2d(ConvolutionalRecurrenceBase):
    """
    Implements a recurrently connected convolutional layer.

    This layer combines feedforward convolutional processing with various types
    of recurrent connections, allowing for dynamic temporal processing and
    feature integration over time.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: Optional[int] = None,
        bias: bool = True,
        dim_y: Optional[int] = None,
        dim_x: Optional[int] = None,
        dt: float = 1,  # ms
        tau: float = 1,  # ms
        recurrence_type: str = "self",
        recurrence_delay: float = 0,  # ms
        recurrence_influence: Union[Callable, str] = "additive",
        history_length: Optional[float] = None,  # ms
        fixed_self_weight: Optional[float] = None,
        max_weight_init: float = 0.001,
        parametrization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
        feedforward_only: bool = False,
        **kwargs,
    ) -> None:
        super().__init__()

        # Store core convolution parameters
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.bias = bias
        self.parametrization = parametrization

        # Store spatial dimensions
        self.dim_y = dim_y
        self.dim_x = dim_x

        # Store recurrence parameters
        self.dt = dt
        self.tau = tau
        self.recurrence_type = recurrence_type
        self.recurrence_delay = recurrence_delay
        self.fixed_self_weight = fixed_self_weight
        self.max_weight_init = max_weight_init
        self.feedforward_only = str_to_bool(feedforward_only)

        # Configure recurrence influence
        self._setup_recurrence_influence(recurrence_influence)

        # Configure hidden state memory
        self._setup_hidden_state_memory(history_length)

        # Only define architecture, initialization happens in setup
        self._define_architecture()
        self.reset()

        # Flag to track initialization
        self._parameters_initialized = False

    def _additive_influence(self, x, h):
        return x + h

    def _multiplicative_influence(self, x, h):
        return x * (1 + torch.tanh(h))

    def _setup_recurrence_influence(self, influence: Union[Callable, str]) -> None:

        if isinstance(influence, str):
            if influence == "additive":
                self.recurrence_influence = self._additive_influence
            elif influence == "multiplicative":
                self.recurrence_influence = self._multiplicative_influence
            else:
                raise ValueError(f"Invalid recurrence influence: {influence}")
        else:
            self.recurrence_influence = influence

    def _setup_hidden_state_memory(self, history_length: Optional[float]) -> None:
        self.history_length = (
            self.recurrence_delay if history_length is None else history_length
        )
        self.n_deque = int(self.history_length / self.dt) + 1
        self.recurrence_delay_i = int(self.recurrence_delay / self.dt)

    def _define_architecture(self) -> None:
        # Define feedforward convolution
        self._setup_feedforward_conv()

        # Define recurrent connection if needed
        if not self.feedforward_only:
            self._setup_recurrence()

    def _setup_feedforward_conv(self) -> None:
        padding = self.kernel_size // 2 if self.padding is None else self.padding

        self.conv = nn.Conv2d(
            in_channels=self.in_channels,
            out_channels=self.out_channels,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=padding,
            bias=self.bias,
        )
        if self.parametrization is not None:
            self.conv = apply_parametrization(self.conv, self.parametrization)

    def _setup_recurrence(self) -> None:
        """Set up the recurrent connection based on specified type."""
        recurrence_params = dict(
            kernel_size=self.kernel_size,
            bias=False,
            parametrization=self.parametrization,
            max_weight_init=self.max_weight_init,
            fixed_weight=self.fixed_self_weight,
            in_channels=self.out_channels,
            dim_y=self.dim_y // self.stride,
            dim_x=self.dim_x // self.stride,
        )

        # Map recurrence types to their implementations
        recurrence_types = {
            "self": SelfConnection,
            "full": FullConnection,
            "depthpointwise": DepthPointwiseConnection,
            "pointdepthwise": PointDepthwiseConnection,
            "local": LocalLateralConnection,
            "localdepthwise": LocalSeparableConnection,
            None: None,
            "none": None,
        }

        if self.recurrence_type.lower() not in recurrence_types:
            raise ValueError(f"Invalid recurrence type: {self.recurrence_type}")

        if self.recurrence_type.lower() in [None, "none"]:
            self.recurrence = None
            return
        else:
            recurrence_class = recurrence_types[self.recurrence_type.lower()]
            self.recurrence = recurrence_class(**recurrence_params)

    def _init_parameters(self) -> None:
        if hasattr(self.conv, "weight.original0"):
            self.conv.weight.original0.data.fill_(1.0)
        if hasattr(self.conv, "weight.original1"):
            nn.init.kaiming_normal_(self.conv.weight.original1, nonlinearity="relu")
        elif hasattr(self.conv, "weight"):
            nn.init.kaiming_normal_(self.conv.weight, nonlinearity="relu")
        if hasattr(self.conv, "bias") and self.conv.bias is not None:
            nn.init.constant_(self.conv.bias, 0)
        if hasattr(self.recurrence, "_init_parameters"):
            self.recurrence._init_parameters()

    def reset(self) -> None:
        self.hidden_states: Deque[torch.Tensor] = deque(maxlen=self.n_deque)

    def get_hidden_state(self, i: Optional[int] = None) -> Optional[torch.Tensor]:
        """
        Get the hidden state at a specific index.

        Args:
            i (Optional[int]): Index of the hidden state. Default is None.

        Returns:
            Optional[torch.Tensor]: Hidden state tensor.
        """
        if i is None:
            return self.hidden_states
        elif i >= 0:
            i -= self.n_deque

        if abs(i) > len(self.hidden_states):
            return None
        else:
            return self.hidden_states[i]

    def set_hidden_state(self, h: torch.Tensor, i: Optional[int] = None) -> None:
        """
        Set the hidden state at a specific index.

        Args:
            h (torch.Tensor): Hidden state tensor.
            i (Optional[int]): Index of the hidden state. Default is None.
        """
        if i is None:
            self.hidden_states.append(h)
        else:
            self.hidden_states[i] = h
        return None

    def forward(
        self,
        x: Optional[torch.Tensor] = None,
        h: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> Optional[torch.Tensor]:
        # Get previous activation for recurrent input
        if h is None:  # passed hidden state takes precedence
            h = self.get_hidden_state(-1 * self.recurrence_delay_i)

        # Response to recurrent input
        if h is None or self.recurrence is None:
            h = None
        else:
            h = self.recurrence(h)

        # Response to feedforward input
        if x is not None:
            x = self.conv(x)

        # Combine responses
        if x is None and h is None:
            out = None
            return out

        elif x is None:
            out = h
        elif h is None:
            out = x
        else:
            out = self.recurrence_influence(x, h)

        return out


if __name__ == "__main__":
    input_shape = (3, 32, 32)
    batch_size = 1
    logger.info("Input shape: %s", (batch_size, *input_shape))

    for class_name in __all__:
        logger.info("Testing %s:", class_name)

        # Create an instance of the class
        module = globals()[class_name]
        if class_name == "RecurrentConnectedConv2d":
            model = module(
                in_channels=input_shape[0],
                out_channels=6,
                kernel_size=3,
                dim_y=input_shape[1],
                dim_x=input_shape[2],
            )
        elif class_name == "InputAdaption":
            model = module()
        elif class_name == "SelfConnection":
            model = module()
        else:
            model = module(in_channels=input_shape[0], kernel_size=3)

        # Print number of parameters
        n_params = sum(p.numel() for p in model.parameters())
        logger.info("Number of parameters: %d", n_params)

        # Print model summary
        if not class_name in ["SelfConnection", "InputAdaption"]:
            print(summary(model, input_shape))

        # Generate random input
        if class_name == "InputAdaption":
            require_grad = True
        else:
            require_grad = False
        random_input = torch.randn(
            batch_size, *input_shape, requires_grad=require_grad
        )

        # Perform forward pass
        output = model(random_input)

        # Define an arbitrary loss function
        loss_fn = nn.MSELoss()

        # Generate random target
        target = torch.randn_like(output)

        # Compute loss
        loss = loss_fn(output, target)

        # Perform backward pass
        loss.backward()

        # Print results
        logger.info(
            "Model Output: %s\n", output.shape if output is not None else "None"
        )

    logger.info("All tests passed!")
