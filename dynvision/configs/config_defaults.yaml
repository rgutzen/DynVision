# Namespace
classifier_name: classifier

# Model
retain_graph: True  # retain the computational graph for repeated backprop paths
status: trained
n_timesteps: 20
dt: 2  # ms
tau: 8  # ms
t_recurrence: 6  # ms
t_feedforward: 10  # ms
recurrence_influence: 'additive'  # 'additive' or 'multiplicative'
init_with_pretrained: False
store_train_responses: 0  # n_samples
store_val_responses: 0  # n_samples
store_test_responses: 100  # n_samples
store_responses_on_cpu: True

# Data
data_name: mnist
data_group: all

# Training run
seed: "0000"
train_ratio: 0.8
epochs: 250
batch_size: 256
check_val_every_n_epoch: 5
log_every_n_steps: 20
accumulate_grad_batches: 2
precision: "bf16-mixed"
profiler: None
benchmark: True
enable_progress_bar: False
# tbptt_length: None
automatic_optimization: True
clip_gradients: False
gradient_clip_algorithm: "norm"  # "norm" or "value"
gradient_clip_val: 1.0  # global clip value
accelerator: "auto"
devices: "auto"
strategy: "auto"
num_nodes: 1
sync_batchnorm: False
deterministic: False  # Consider setting to true if still unstable

use_distributed: True
distributed:
  strategy: "ddp"  # Use standard DDP for SLURM compatibility
  devices: 4  # Number of GPUs per node
  num_nodes: 1  # Number of compute nodes
  sync_batchnorm: True
  precision: "bf16-mixed"  # Options: 32, 16-mixed, bf16-mixed
  accelerator: "gpu"  # Explicitly set GPU accelerator
  # Advanced DDP settings
  find_unused_parameters: False
  gradient_as_bucket_view: True
  process_group_backend: "nccl"  # Use NCCL for GPU communication


# Debugging settings
debug_batch_size: 3
debug_check_val_every_n_epoch: 1
debug_log_every_n_steps: 1
debug_accumulate_grad_batches: 1
debug_enable_progress_bar: True

# Test run
logger: False

# Loss
loss: 
  - CrossEntropyLoss
loss_configs:
  CrossEntropyLoss:
    weight: 1
    ignore_index: -100
  EnergyLoss:
    weight: 100
loss_reaction_time: 0  # ms (+ residual time)
non_label_index: -100

# Optimizer
optimizer: Adam
optimizer_kwargs:
  weight_decay: 0.0005
  fused: False
optimizer_configs:
  monitor: train_loss

# Learning rate parameter groups
lr_parameter_groups:
  regular:
    lr_factor: 1.0
  recurrence:
    lr_factor: 0.2
  feedback:
    lr_factor: 0.5

# Learning rate
learning_rate: 0.002
scheduler: CosineAnnealingLR
scheduler_kwargs:
  T_max: 250
  eta_min: 0.0001
scheduler_configs:
  interval: epoch
  frequency: 1
  monitor: train_loss
  name: lr_scheduled
lr_scaling:
  enabled: False  # whether to scale learning rate with batch size
  ref_batch_size: 64  # reference batch size for scaling
  threshold_factor: 8  # maximum scaling threshold

# Trainer configuration
# Gradient clipping settings

# System
log_level: info
use_executor: False
executor_start: "singularity exec --nv \
  --overlay /scratch/rg5022/images/rva.ext3:ro \
  --overlay /vast/work/public/ml-datasets/imagenet/imagenet-train.sqf:ro \
  --overlay /vast/work/public/ml-datasets/imagenet/imagenet-val.sqf:ro \
  --overlay /vast/work/public/ml-datasets/imagenet/imagenet-test.sqf:ro \
  /scratch/work/public/singularity/cuda12.2.2-cudnn8.9.4-devel-ubuntu22.04.3.sif \
  bash -c '\
  source /ext3/env.sh; \
  conda activate rva; \
  "
executor_close: "'"
verbose: True