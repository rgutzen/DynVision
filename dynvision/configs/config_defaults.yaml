# Namespace
classifier_name: classifier

# Model
status: trained
n_timesteps: 20
dt: 2  # ms
# tau: 5  # ms
# t_recurrence: 6  # ms
# t_feedforward: 0  # ms
# t_skip: 0  # ms
# t_feedback: 34  # ms
# loss_reaction_time: 4  # ms 
# recurrence_type: 'full'
# recurrence_target: 'output'
# integration_strategy: 'additive'  # 'additive' or 'multiplicative'
# dynamics_solver: "euler" 
# skip: True
# feedback: False
# init_with_pretrained: False
store_train_responses: 0 # n_batches
store_val_responses: 0  # n_batches
store_test_responses: 5  # n_batches
store_responses_on_cpu: True
target_dtype: "float16"
retain_graph: False  # retain the computational graph for repeated backprop paths

# Data
data_name: mnist
data_group: all
data_timesteps: 1

# Training run
seed: "0000"
train_ratio: 0.9
epochs: 100
check_val_every_n_epoch: 10
log_every_n_steps: 20
accumulate_grad_batches: 4
num_sanity_val_steps: 0
precision: "bf16-mixed"
profiler: None
benchmark: True
enable_progress_bar: False
limit_val_batches: 0.2
reload_dataloaders_every_n_epochs: 0
automatic_optimization: True
clip_gradients: False
gradient_clip_algorithm: "norm"  # "norm" or "value"
gradient_clip_val: 1.0  # global clip value
deterministic: False  # Consider setting to true if unstable
devices: 1

# DataLoader
batch_size: 256
test_batch_size: 64
order: "QUASI_RANDOM"
os_cache: True
# chunksize: 5000
drop_last: True  # Ensure all batches are of the same size
use_ffcv: True
num_workers: 4
persistent_workers: True
pin_memory: False
prefetch_factor: 1

# Test run
logger: False

# Loss
loss: 
  - CrossEntropyLoss
loss_configs:
  CrossEntropyLoss:
    weight: 1
    ignore_index: -1
  EnergyLoss:
    weight: 0.05
    p: 1
loss_reaction_time: 0  # ms (+ residual time)
non_label_index: -1
non_input_value: 0

# Optimizer
optimizer: Adam
optimizer_kwargs:
  weight_decay: 0.0005
  fused: False
optimizer_configs:
  monitor: val_loss

# Learning rate parameter groups
lr_parameter_groups:
  regular:
    lr_factor: 1.0
  recurrence:
    lr_factor: 0.2
  feedback:
    lr_factor: 0.5

# Learning rate
learning_rate: 0.0002
scheduler: CosineAnnealingLR
scheduler_kwargs:
  T_max: 250
scheduler_configs:
  interval: epoch
  frequency: 1
  monitor: train_loss
  name: lr_scheduled
lr_scaling:
  enabled: False  # whether to scale learning rate with batch size
  ref_batch_size: 64  # reference batch size for scaling
  threshold_factor: 8  # maximum scaling threshold

# System
log_level: info
use_executor: False  # automatic setting (don't change)
verbose: True