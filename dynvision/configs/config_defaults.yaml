# ===== NAMESPACE =====
classifier_name: classifier

# ===== MODEL STATUS =====
status: trained

# ===== CORE ARCHITECTURE =====
# # Framework defaults - models can override in their __init__
# n_classes: 10  # Number of output classes
# input_dims: [1, 3, 224, 224]  # (timesteps, channels, height, width)
# n_timesteps: 20  # Number of temporal timesteps
# data_presentation_pattern: [1]  # 1 = present input, 0 = blank
# input_adaption_weight: 0.0  # Weight for consecutive timestep inputs

# # ===== BIOLOGICAL PARAMETERS =====
# dt: 2  # ms - integration time step
# tau: 10  # ms - neural time constant
# t_feedforward: 0  # ms - feedforward delay
# t_recurrence: 6  # ms - recurrent delay
# t_feedback: 34  # ms - feedback delay
# t_skip: 0  # ms - skip connection delay
# dynamics_solver: "euler"  # "euler" or "rk4"
# idle_timesteps: 0  # Idle timesteps for convergence
# feedforward_only: False  # Use only feedforward connections

# # ===== RECURRENT ARCHITECTURE =====
# recurrence_type: "none"  # "full", "self", "depthwise", etc.

# # ===== CONNECTIVITY =====
# skip: False  # Enable skip connections
# feedback: False  # Enable feedback connections

# # ===== NONLINEARITIES =====
# supralinearity: 1  # Supralinearity exponent

# ===== RESPONSE STORAGE =====
store_responses: 0  # DEPRECATED: use store_test_responses
store_train_responses: 0  # n_batches
store_val_responses: 0  # n_batches
store_test_responses: 5  # n_batches
store_responses_on_cpu: True  # Store on CPU to save GPU memory
early_test_stop: True  # Stop testing early when buffer filled

# ===== LOSS CONFIGURATION =====
loss: CrossEntropyLoss  # Loss function name
loss_reaction_time: 0  # ms - reaction time for loss calculation
non_label_index: -1  # Index for non-label timesteps

# ===== TRAINING BEHAVIOR =====
target_dtype: "float16"  # Target dtype for model outputs
retain_graph: False  # Retain computational graph for repeated backprop

# Data
data_name: mnist
data_group: all
data_timesteps: 1

# Training run
seed: "0000"
train_ratio: 0.9
epochs: 100
check_val_every_n_epoch: 10
log_every_n_steps: 20
accumulate_grad_batches: 4
num_sanity_val_steps: 0
precision: "bf16-mixed"
profiler: None
benchmark: True
enable_progress_bar: False
limit_val_batches: 0.2
reload_dataloaders_every_n_epochs: 0
automatic_optimization: True
clip_gradients: False
gradient_clip_algorithm: "norm"  # "norm" or "value"
gradient_clip_val: 1.0  # global clip value
deterministic: False  # Consider setting to true if unstable
devices: 1

# DataLoader
batch_size: 256
test_batch_size: 64
order: "QUASI_RANDOM"
os_cache: True
# chunksize: 5000
drop_last: True  # Ensure all batches are of the same size
use_ffcv: True
num_workers: 4
persistent_workers: True
pin_memory: False
prefetch_factor: 1

# Test run
logger: False

# ===== LOSS CONFIGURATION (DETAILED) =====
loss:
  - CrossEntropyLoss
loss_configs:
  CrossEntropyLoss:
    weight: 1
    ignore_index: -1
  EnergyLoss:
    weight: 0.05
    p: 1
non_input_value: 0

# ===== OPTIMIZER CONFIGURATION =====
optimizer: Adam
optimizer_kwargs:
  weight_decay: 0.0005
  fused: False
optimizer_configs:
  monitor: val_loss

# ===== LEARNING RATE PARAMETER GROUPS =====
lr_parameter_groups:
  regular:
    lr_factor: 1.0
  recurrence:
    lr_factor: 0.2
  feedback:
    lr_factor: 0.5

# ===== LEARNING RATE & SCHEDULER =====
learning_rate: 0.0002
scheduler: CosineAnnealingLR
scheduler_kwargs:
  T_max: 250
scheduler_configs:
  interval: epoch
  frequency: 1
  monitor: train_loss
  name: lr_scheduled
lr_scaling:
  enabled: False  # whether to scale learning rate with batch size
  ref_batch_size: 64  # reference batch size for scaling
  threshold_factor: 8  # maximum scaling threshold

# System
log_level: info
use_executor: False  # automatic setting (don't change)
verbose: True