# Local Settings
use_local_mode: False
local:
  enable_progress_bar: True
  use_debug_mode: False
  use_distributed_mode: False
  verbose: True
  use_ffcv: False 
  num_workers: 0
  persistent_workers: False

# Debugging Settings
use_debug_mode: False
debug:
  batch_size: 3
  check_val_every_n_epoch: 1
  log_every_n_steps: 1
  accumulate_grad_batches: 1
  enable_progress_bar: True

# Large Dataset Settings
use_large_dataset_mode: False
large_dataset:
  # batch_size: 32
  use_ffcv: True
  order: "QUASI_RANDOM"
  persistent_workers: False
  num_workers: 4
  precision: "bf16-mixed"
  pin_memory: False
  os_cache: False
  chunksize: 5000

# Distributed Settings
use_distributed_mode: False
distributed:
  use_distributed: False  # For FFCV dataloaders - let lightning handle distribution
  strategy: "ddp"  # Use standard DDP for SLURM compatibility
  devices: 4  # Number of GPUs per node
  num_nodes: 1  # Number of compute nodes
  sync_batchnorm: True
  precision: "bf16-mixed"  # Options: 32, 16-mixed, bf16-mixed
  accelerator: "gpu"  # Explicitly set GPU accelerator
  accumulate_grad_batches: 2
  num_sanity_val_steps: 0
  store_train_responses: 0  # n_batches
  store_val_responses: 0  # n_batches
  check_val_every_n_epoch: 100
  batch_size: 256
  use_ffcv: True
  num_workers: 8
  persistent_workers: True
  pin_memory: False
  order: "QUASI_RANDOM"
  os_cache: True
  chunksize: 5000
  drop_last: True  # Ensure all batches are of the same size
  # Advanced DDP settings
  strategy_kwargs:
    find_unused_parameters: False  # Set to True if your model has unused parameters
    gradient_as_bucket_view: True  # Use bucket views for gradients
    process_group_backend: "nccl"  # Use NCCL for GPU communication
    static_graph: False
  # FSDP settings
  # strategy_kwargs:
  #   state_dict_type: "sharded"
  #   sharding_strategy: "NO_SHARD"

  
